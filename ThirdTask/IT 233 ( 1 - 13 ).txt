
College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 1: Computers and Systems


Typical Computer Ad

‚Ä¢ Is the computer fast enough to

run necessary programs?

‚Ä¢ Is the computer cost-effective?

‚Ä¢ Will it be obsolete in 6 months?


Why Study Computer System Architecture?

‚Ä¢ User

‚Ä¢ Understand system capabilities and limitations

‚Ä¢ Make informed decisions

‚Ä¢ Improve communications with information technology professionals

‚Ä¢ Programmer

‚Ä¢ Create efficient application software for specific processing needs

‚Ä¢ Systems Architect or Systems Analyst

‚Ä¢ Specify computer systems and architecture to meet application requirements

‚Ä¢ Make intelligent decisions about system strategy


Why Study Computer System Architecture?

‚Ä¢ System Administrator / Manager

‚Ä¢ Install, configure, maintain, and upgrade computer systems

‚Ä¢ Maximize system availability and efficiency

‚Ä¢ Optimize system performance

‚Ä¢ Ensure system security

‚Ä¢ Web Services Designer

‚Ä¢ Optimize customer accessibility to Web services

‚Ä¢ Optimize web system configurations

‚Ä¢ Select appropriate data formats, page designs and scripting
languages

‚Ä¢ Design efficient Web pages


Web Browser Application Use


Input-Process-Output Model (IPO)

‚Ä¢  Input: keyboard, mouse, scanner, punch cards

‚Ä¢  Processing: CPU executes the computer program

‚Ä¢ Output: monitor, printer, fax machine

‚Ä¢  Storage: hard drive, optical media, diskettes, magnetic tape


Simplified IT Computer System Layout


Computer System Components

‚Ä¢ Hardware

‚Ä¢ Processes data by executing instructions

‚Ä¢ Provides input and output

‚Ä¢ Control input, output and storage components

‚Ä¢ Software

‚Ä¢ Applications and system software

‚Ä¢ Instructions tell hardware exactly what tasks to perform and in what order

‚Ä¢ Data

‚Ä¢ Fundamental representation of facts and observations

‚Ä¢ Communications

‚Ä¢ Sharing data and processing among different systems


Hardware Component

‚Ä¢ Input/Output devices

‚Ä¢ Storage Devices

‚Ä¢ CPU ‚Äì Central Processing Unit

‚Ä¢ ALU: arithmetic/logic unit

‚Ä¢ CU: control unit

‚Ä¢ Interface unit

‚Ä¢ Memory

‚Ä¢ Short-term storage for CPU calculations


Typical Personal Computer System


CPU: Central Processing Unit

‚Ä¢ ALU: arithmetic/logic unit

‚Ä¢ Performs arithmetic and Boolean logical calculations

‚Ä¢ CU: control unit

‚Ä¢ Controls processing of instructions

‚Ä¢ Controls movement of data within the CPU

‚Ä¢ Interface unit

‚Ä¢ Moves instructions and data between the CPU and other hardware
components

‚Ä¢ Bus: bundle of wires that carry signals and power between different
components


Memory

‚Ä¢ Also known as primary storage, working storage, working
storage, and RAM (random access memory)

‚Ä¢ Consists of bits, each of which hold a value of either 0 or 1
(8 bits = 1 byte)

‚Ä¢ Holds both instructions and data of a computer program
(stored program concept)


Software Component

‚Ä¢ Applications

‚Ä¢ Operating System

‚Ä¢ API: application program
interface

‚Ä¢ File management

‚Ä¢ I/O

‚Ä¢ Kernel

‚Ä¢  Memory management

‚Ä¢  Resource scheduling

‚Ä¢  Program communication

‚Ä¢  Security

‚Ä¢ Network Module


Communications Component

‚Ä¢ Hardware

‚Ä¢ Communication channels

‚Ä¢  Physical connections between computer systems

‚Ä¢  Examples: wire cable, phone lines, fiber optic cable, infrared light, radio

waves

‚Ä¢ Interface hardware

‚Ä¢  Handles communication between the computer and the communication
channel

‚Ä¢  Modem or network interface card (NIC)

‚Ä¢ Software

‚Ä¢ Establish connections

‚Ä¢ Control flow of data

‚Ä¢ Directs data to the proper applications for use


Computer Systems

All computer systems, no matter how complex,
consists of the following:

‚Ä¢ At least one CPU

‚Ä¢ Memory to hold programs and data

‚Ä¢ I/O devices

‚Ä¢ Long-term storage


Computer Systems Examples

HP Laptop Computer

IBM System z10 EC Mainframe


Virtualization

‚Ä¢ Virtual (American Heritage Dictionary(

‚Ä¢ Existing or result in essence or effect though not in
actual fact, form or name

‚Ä¢ Created, simulated, or carried on by means of a
computer or computer network

‚Ä¢ Computer systems examples

‚Ä¢ Virtual memory

‚Ä¢ Virtual networks

‚Ä¢ Java Virtual Machine


Protocols

‚Ä¢ Common ground rules of communication between
computers, I/O devices, and many software programs

‚Ä¢ Examples

‚Ä¢ HTTP: between Web servers and Web browsers

‚Ä¢ TCP/IP: between computers on the Internet and local
area networks

‚Ä¢ SATA: between storage devices and computers

‚Ä¢ XML,RSS, SIP: new protocols



Standards

‚Ä¢ Created to

ensure universal compatibility of data

formats and protocols

‚Ä¢ May be created by committee or may become a de facto
standard through popular use

‚Ä¢ Examples:

‚Ä¢ Computer languages: Java, SQL, C, JavaScript

‚Ä¢ Display standards: Postscript, MPEG-2, JPEG, GIF

‚Ä¢ Character set standards: ASCII, Unicode, EBCDIC

‚Ä¢ Multimedia standards: MPEG-2, MPEG-4, DivX, MP3


Textbook Overview

‚Ä¢ Web site: http://www.wiley.com/college/englander

‚Ä¢ Part 1 (Chapters 1-2)

‚Ä¢ Overview of computer systems

‚Ä¢ Part 2 (Chapters 3-5)

‚Ä¢ Number systems and data formats

‚Ä¢ Part 3 (Chapters 6-11)

‚Ä¢ Computer architecture and hardware operation

‚Ä¢ Part 4 (Chapters 12-14)

‚Ä¢ Networks and data communications

‚Ä¢ Part 5 (Chapters 15-18)

‚Ä¢ Software component ‚Äì operating systems

‚Ä¢ Part 6 (Supplementary Chapters S1-S4)

‚Ä¢ Digital logic, systems examples, instruction addressing modes, programming

tools


Early History

‚Ä¢ 1642: Blaise Pascal invents a calculating machine

‚Ä¢ 1801: Joseph Marie Jacquard invents a loom that uses punch
cards

‚Ä¢ 1800‚Äôs:

‚Ä¢ Charles Babbage attempts to build an analytical engine (mechanical
computer)

‚Ä¢ Augusta Ada Byron develops many of the fundamental concepts of
programming

‚Ä¢ George Boole invents Boolean logic.


Modern Computer Development

‚Ä¢ 1937: Mark I is built (Aiken, Harvard University, IBM).

‚Ä¢ First electronic computer using relays.

‚Ä¢ 1939: ABC is built

‚Ä¢ First fully electronic digital computer. Used vacuum tubes.

‚Ä¢ 1943-46: ENIAC (Mauchly, Eckert, University of Pennsylvania).

‚Ä¢ First general purpose digital computer.

‚Ä¢ 1945: Von Neumann architecture proposed.

‚Ä¢ Still the standard for present day computers.

‚Ä¢ 1947: Creation of transistor

‚Ä¢ (Bardeen, Shockley, Brattain, Bell Labs).

‚Ä¢ 1951-2: EDVAC and IAS


Early Computers

Babbage‚Äôs Analytical Engine                 ENIAC


System Software History

‚Ä¢ Early computers had no operating systems and were single user systems

‚Ä¢ Programs were entered using switches for each bit or by plugging wires into a

panel

‚Ä¢ 1953-54: First operating system was built by General Motors Research
Laboratories for their IBM 701 computer

‚Ä¢ Other early systems

‚Ä¢ FORTRAN Monitor System (FMS)

‚Ä¢ IBSYS

‚Ä¢ Share Operating System (SOS)


Operating System Development

‚Ä¢ 1963: Master Control Program (MCP) by Burroughs. Included
many modern OS features.

‚Ä¢ 1964: OS/360 by IBM. Included batch processing of programs.

‚Ä¢ 1962: MIT Project MAC created a time-sharing OS called CTSS.
Shortly afterwards, MIT, Bell Labs, and GE developed Multics
(Multiplexed Information and Computing Services).


UNIX

‚Ä¢ After Bell Labs withdrew from the Multics project, Ken Thompson developed a
personal operating system called UNIX using assembly language.

‚Ä¢ Dennis Ritchie developed the programming language C which was used to

rewrite much of UNIX in a high-level language.

‚Ä¢ UNIX introduced

‚Ä¢ A hierarchical file system

‚Ä¢ The shell concept

‚Ä¢ Document production and formatting

‚Ä¢ Tools for networked and distributed processing


Graphical User Interfaces

‚Ä¢ 1960s: Doug Englebart (Stanford Research Institute)

‚Ä¢ Invented windows and a mouse interface

‚Ä¢ 1970s: Xerox PARC

‚Ä¢ Creates a practical windowing system for the Dynabook
project

‚Ä¢ 1980s: Steve Jobs (Apple)

‚Ä¢ Developed the Apple Lisa and MacIntosh


IBM PC

‚Ä¢ 1982: Stand-alone, single user computer

‚Ä¢ PC-DOS, MS-DOS (disk operating system)

‚Ä¢ Later versions of DOS added

‚Ä¢ Hierarchical directory file storage

‚Ä¢ File redirection

‚Ä¢ Better memory management

‚Ä¢ Windowing systems

‚Ä¢ Windows 2.0, Windows 3.1, Windows 95

‚Ä¢ Windows NT, Windows XP, Windows Vista

‚Ä¢ Windows 7


Communications

‚Ä¢ 1960s and 1970s: users communicated on multiterminal computer systems using
talk and email facilities

‚Ä¢ 1971: Ray Tomlinson creates the standard username@hostname email standard

‚Ä¢ Modems permitted users to login to office systems, electronic bulletin board
systems, Compuserve, AOL, and Prodigy

‚Ä¢ 1969: ARPANET begun

‚Ä¢ 1985: First TCP-IP wide area network

‚Ä¢ 1991: Tim Berners Lee develops the concepts that become the World Wide Web

‚Ä¢ 1993: Max Andreessen develops Mosaic, the first graphical browser


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work
beyond that permitted in section 117 of the 1976 United
States Copyright Act without express permission of the
copyright owner is unlawful. Request for further information
should be addressed to the Permissions Department, John
Wiley & Sons, Inc. The purchaser may make back-up copies
for his/her own use only and not for distribution or resale.

The Publisher assumes no responsibility for errors,

omissions, or damages caused by the use of these programs
or from the use of the information contained herein.‚Äù


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 2:

Introduction to Systems Concepts and Systems
Architecture


What is a system?

‚Ä¢ What do the following systems have in common?

1. Plumbing system

2. Solar system

3. Home network system

4. Inventory control system


Plumbing System


Solar System


Home Network System


Inventory Control System


Definition of a System

‚Ä¢ ‚ÄúA systems is a collection of components linked together and
organized in such a way as to be recognizable as a single unit.‚Äù

‚Ä¢ Linked components of a system also define the boundary for
the system

‚Ä¢ The environment is anything outside of the system


General Representation of a System


System Decomposition

‚Ä¢ Components

‚Ä¢ May be irreducible or

‚Ä¢ May be subsystems

‚Ä¢ Decomposition

‚Ä¢ The division of a system into its components and linkages

‚Ä¢ Hierarchical


System Architecture

‚ÄúThe fundamental properties, and the patterns of
relationships, connections, constraints, and linkages among
the components and between the system and its
environment are known collectively as the architecture of
the  system‚Äù


Abstractions of Systems

‚Ä¢ How are the following two abstractions of a business system
different from one another?

‚Ä¢ How are these abstractions different from the real business
system?


Business Organization Chart


Business Application Architecture


IT System Architectures

‚Ä¢ Distributed processing systems

‚Ä¢ Client-Server Computing

‚Ä¢ 2-tier architecture(Client + Data Base)

‚Ä¢ 3-tier architecture(Client + Server+Data Base)

‚Ä¢ N-tier architecture

‚Ä¢ Web-Based Computing

‚Ä¢ Peer-to-Peer Computing


Client-Server Computing

‚Ä¢ A program on a client computer requests services from a program
on a server computer

‚Ä¢ Examples:

‚Ä¢ Email services, file services, print services, directory services,
Web services, database services, application services, remote
access services


Basic Client-Server Architecture


Advantages of Client-Server Architecture

‚Ä¢ Centralization of services permits

‚Ä¢ easier administration of services by IT professionals

‚Ä¢ easier availability and location by users

‚Ä¢ consistency of resources, such as files and data, can be
managed and assured

‚Ä¢ more efficient and cost-effective hardware procurement
through purchasing a small number of very powerful
computers


Clients and Servers on a Network


Multi-tier Architectures

‚Ä¢ Two-tier architecture

‚Ä¢ Two computers are involved in a service.

‚Ä¢ Example: Web-browser and Web server model used in
intranets and on the Internet

‚Ä¢ Three-tier architecture

‚Ä¢ Three computers are involved in a service

‚Ä¢ Example: client computer, Web server, database server

‚Ä¢ N-tier architecture


Three-tier Architecture


Peer-to-Peer Computing

‚Ä¢ Computers on a network are treated as equals

‚Ä¢ Each computer can share resources with the other
computers on the network


Peer-to-Peer Computing

‚Ä¢ Disadvantages

‚Ä¢ Difficult to establish centralized control of services

‚Ä¢ Difficult to locate services

‚Ä¢ Difficult to synchronize versions of files or software

‚Ä¢ Difficult to secure network from unauthorized access and from
viruses

‚Ä¢ Advantages

‚Ä¢ Sharing files between personal computers

‚Ä¢ Internet file sharing


Hybrid Model of Computing

‚Ä¢ Client-server technology used to locate systems and files

‚Ä¢ Then systems can participate in peer-to-peer transactions

‚Ä¢ Examples

‚Ä¢ Instant messaging

‚Ä¢ Skype

‚Ä¢ Napster


Google: System Architecture

‚Ä¢ Provide powerful, fast search capability for material on the Internet

‚Ä¢ Derive income from advertising that is targeted to each user based on
their searches


Google: System Architecture

‚Ä¢ Basic requirements

‚Ä¢ Capable of responding to millions of simultaneous requests from
all over the world

‚Ä¢ Perform a web crawl of the Internet retrieve and organize data

‚Ä¢ Establish ranking of results with appropriately targeted
advertising

‚Ä¢ High reliability of the system

‚Ä¢ System is easily scalable and cost effective


Google Data Center Search Application

Architecture


Simplified Google System Hardware
Architecture


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work
beyond that permitted in section 117 of the 1976 United
States Copyright Act without express permission of the
copyright owner is unlawful. Request for further information
should be addressed to the Permissions Department, John
Wiley & Sons, Inc. The purchaser may make back-up copies
for his/her own use only and not for distribution or resale.

The Publisher assumes no responsibility for errors,

omissions, or damages caused by the use of these programs
or from the use of the information contained herein.‚Äù


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 3: Number

Systems

The Architecture of Computer Hardware and Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong, Bentley University

PowerPoint slides for the 3rd edition were co-authored with Lynne Senne, Bentley University


Why Binary?

‚Ä¢ Early computer design was decimal

‚Ä¢ Mark I and ENIAC

‚Ä¢ John von Neumann proposed binary data
processing (1945)

‚Ä¢ Simplified computer design

‚Ä¢ Used for both instructions and data

‚Ä¢ Natural relationship between


on/off switches and

calculation using Boolean logic

Copyright 2010 John Wiley & Sons, Inc.

On
True
Yes
1

Off
False
No

0


Counting and Arithmetic

‚Ä¢ Decimal or base 10 number system

‚Ä¢ Origin: counting on the fingers

‚Ä¢ ‚ÄúDigit‚Äù from the Latin word digitus meaning ‚Äúfinger‚Äù

‚Ä¢ Base: the number of different digits including zero in the number system

‚Ä¢ Example: Base 10 has 10 digits, 0 through 9

‚Ä¢ Binary or base 2

‚Ä¢ Bit (binary digit): 2 digits, 0 and 1

‚Ä¢ Octal or base 8: 8 digits, 0 through 7

‚Ä¢ Hexadecimal or base 16:

16 digits, 0 through F

‚Ä¢ Examples: 1010 = A16; 1110 = B16


Copyright 2010 John Wiley & Sons, Inc.

3-6


Keeping Track of the Bits

‚Ä¢ Bits commonly stored and manipulated in groups

‚Ä¢ 8 bits = 1 byte

‚Ä¢ 4 bytes = 1 word (in many systems)

‚Ä¢ Number of bits used in calculations

‚Ä¢ Affects accuracy of results

‚Ä¢ Limits size of numbers manipulated by the computer


Copyright 2010 John Wiley & Sons, Inc.

3-7


Numbers: Physical Representation

‚Ä¢ Different numerals, same
number of oranges

‚Ä¢ Cave dweller: IIIII

‚Ä¢ Roman: V

‚Ä¢ Arabic: 5

‚Ä¢ Different bases, same
number of oranges

‚Ä¢   510

‚Ä¢ 1012

‚Ä¢  123

Copyright 2010 John Wiley & Sons, Inc.


Number System

‚Ä¢ Roman: position independent

‚Ä¢ Modern: based on positional notation (place value)

‚Ä¢ Decimal system: system of positional notation based on powers of 10.

‚Ä¢ Binary system: system of positional notation based powers of 2

‚Ä¢ Octal system: system of positional notation based on powers of 8

‚Ä¢ Hexadecimal system: system of positional notation based powers of 16


Copyright 2010 John Wiley & Sons, Inc.

3-9


Positional Notation: Base 10

527 = 5 x 102 + 2 x 101 + 7 x 100


100‚Äôs place

10‚Äôs place

1‚Äôs place


Place
Value
Evaluate
Sum

102

100

5 x 100

500

101

10

2 x 10

20

100

1

7 x1

7


Copyright 2010 John Wiley & Sons, Inc.

3-10


Positional Notation: Octal

6248 = 40410

64‚Äôs place  8‚Äôs place               1‚Äôs place


Place

Value

Evaluate

Sum for
Base 10

82

64

6 x 64

384

81

8

2 x 8

16

80

1

4 x 1

4


Copyright 2010 John Wiley & Sons, Inc.

3-11


Positional Notation:
Hexadecimal

6,70416 = 26,37210

4,096‚Äôs place  256‚Äôs place

16‚Äôs place

1‚Äôs place


Place
Value
Evaluate

Sum for

Base 10

163

4,096

6 x

4,096

24,576

162

256

7 x 256

1,792

161

16

0 x 16

0

160

1

4 x 1

4


Copyright 2010 John Wiley & Sons, Inc.

3-12


Positional Notation: Binary

1101 0110‚ÇÇ = 214‚ÇÅ‚ÇÄ


Place

27     26

25     24

23    22

21    20


Value

128    64

32    16     8     4     2     1


Evaluate

1 x 128

1 x 64

0 x 32

1 x16

0 x 8

1 x 4

1 x 2

0 x 1


Sum for

Base 10

128    64

0     16     0     4     2     0

Copyright 2010 John Wiley & Sons, Inc.


Range of Possible Numbers

‚Ä¢ R = BK where

‚Ä¢ R = range

‚Ä¢ B = base

‚Ä¢ K = number of digits

‚Ä¢ Example #1: Base 10, 2 digits

‚Ä¢ R = 10¬≤ ‚Åº 100 different numbers (0‚Ä¶99)

‚Ä¢ Example #2: Base 2, 16 digits

‚Ä¢ R = 2¬π‚Å∂ = 65,536 or 64K

‚Ä¢ 16-bit PC can store 65,536 different number values

Copyright 2010 John Wiley & Sons, Inc.


Decimal Range for Bit Widths


Bits

1

4

8

10

16

20

32

64

128

Digits

0+

1+

2+

3

4+

6

9+

19+

38+

Range

2 (0 and 1)

16 (0 to 15)

256

1,024 (1K)

65,536 (64K)

1,048,576 (1M)

4,294,967,296 (4G)

Approx. 1.6 x 1019

Approx. 2.6 x 1038

Copyright 2010 John Wiley & Sons, Inc.


Base or Radix

‚Ä¢ Base:

‚Ä¢ The number of different symbols required to represent
any given number

‚Ä¢ The larger the base, the more numerals are
required

‚Ä¢ Base 10:  0,1, 2,3,4,5,6,7,8,9

‚Ä¢ Base  2:   0,1

‚Ä¢ Base  8:   0,1,2, 3,4,5,6,7

‚Ä¢ Base 16:  0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F


Copyright 2010 John Wiley & Sons, Inc.

3-16


Number of Symbols
vs. Number of Digits

‚Ä¢ For a given number, the larger the base

‚Ä¢ the more symbols required

‚Ä¢ but the fewer digits needed

‚Ä¢ Example #1:

‚Ä¢ 65‚ÇÅ‚ÇÜ    101‚ÇÅ‚ÇÄ  145‚Çà   110 0101‚ÇÇ

‚Ä¢ Example #2:

‚Ä¢ 11C‚ÇÅ‚ÇÜ   284‚ÇÅ‚ÇÄ  434‚Çà   1 0001 1100‚ÇÇ


Copyright 2010 John Wiley & Sons, Inc.

3-17


Counting in Base 2


Binary

Number

0

1

10

11

100

101

110

111

1000

1001

1010

Equivalent

8‚Äôs (23)    4‚Äôs (22)    2‚Äôs (21)    1‚Äôs (20)

0 x 20

1 x 20

1 x 21      0 x 20

1 x 21      1 x 20

1 x 22

1 x 22                 1 x 20

1 x 22      1 x 21

1 x 22      1 x 21      1 x 20

1 x 23

1 x 23                            1 x 20

1 x 23                 1 x 21

Decimal

Number

0

1

2

3

4

5

6

7

8

9

10

Copyright 2010 John Wiley & Sons, Inc.


Base 10 Addition Table

310 + 610 = 910

+    0    1   2    3   4   5   6   7   8   9

0    0    1   2    3   4   5   6   7   8   9

1    1    2   3    4   5   6   7   8   9   10

2    2    3   4    5   6   7   8   9   10   11

3    3    4   5    6   7   8   9   10   11   12


4    4    5   6    7   8

etc

9  10  11  12  13

Copyright 2010 John Wiley & Sons, Inc.


Base 8 Addition Table

38 + 68 = 118

+     0    1    2    3    4    5    6    7

0     0    1    2    3    4    5    6    7


1     1    2    3    4    5    6    7   10

2     2    3    4    5    6    7   10   11

3     3    4    5    6    7   10   11   12

(no 8 or 9,
of course)

4     4    5    6    7   10   11   12   13

5     5    6    7    10   11   12   13   14

6     6    7   10    11   12   13   14   15

7     7   10   11    12   13   14   15   16

Copyright 2010 John Wiley & Sons, Inc.


Base 10 Multiplication Table

310 x 610 = 1810

x     0    1    2    3    4    5    6    7    8    9

0                  0

1          1    2    3    4    5    6    7    8    9

2          2    4    6    8   10   12   14   16   18

3          3    6    9   12   15   18   21   24   27

4     0    4    8    12   16   20   24   28   32   36

5          5   10    15   20   25   30   35   40   45

6          6   12    18   24   30   36   42   48   54


7

Copyright 2010 John Wiley & Sons, Inc.

7   14    21   28   35   42   49   56   63

etc.


Base 8 Multiplication Table

38 x 68 = 228

x     0    1    2    3    4    5    6    7

0                  0

1          1    2    3    4    5    6    7

2          2    4    6   10   12   14   16

3     0    3    6    11   14   17   22   25

4          4   10    14   20   24   30   34

5          5   12    17   24   31   36   43

6          6   14    22   30   36   44   52

7          7   16    25   34   43   52   61

Copyright 2010 John Wiley & Sons, Inc.


Addition


Base

Problem

Largest Single Digit

6

Decimal        +3           9

6

Octal          +1           7

6

Hexadecimal     +9           F

1

Binary         +0           1

Copyright 2010 John Wiley & Sons, Inc.


Addition


Base

Decimal

Octal
Hexadecimal
Binary

Problem

6

+4

6

+2

6

+A

1

+1

Carry

Carry the 10

Carry the 8

Carry the 16

Carry the 2

Answer

10

10

10

10

Copyright 2010 John Wiley & Sons, Inc.


Binary Arithmetic

1   1     1     1     1

1   1   0   1   1   0   1

+           1   0   1   1   0

1   0   0   0   0   0   1   1

Copyright 2010 John Wiley & Sons, Inc.


Binary Arithmetic

‚Ä¢ Addition

‚Ä¢ Boolean using XOR
and AND

‚Ä¢ Multiplication

‚Ä¢ AND

‚Ä¢ Shift

‚Ä¢ Division

+    0     1

0   0     1

1   1    10

x    0     1

0   0     0

1   0     1


Copyright 2010 John Wiley & Sons, Inc.

3-26


Binary Arithmetic: Boolean Logic

‚Ä¢ Boolean logic without performing arithmetic

‚Ä¢ EXCLUSIVE-OR

‚Ä¢  Output is ‚Äú1‚Äù only if either input, but not both inputs, is a ‚Äú1‚Äù

‚Ä¢ AND (carry bit)

‚Ä¢  Output is ‚Äú1‚Äù if and only both inputs are a ‚Äú1‚Äù

1   1      1     1     1

1    1    0    1    1    0    1

+            1    0    1    1    0

1    0    0    0    0    0    1    1

Copyright 2010 John Wiley & Sons, Inc.


Binary Multiplication

‚Ä¢ Boolean logic without performing arithmetic

‚Ä¢ AND (carry bit)

‚Ä¢  Output is ‚Äú1‚Äù if and only both inputs are a ‚Äú1‚Äù

‚Ä¢ Shift

‚Ä¢  Shifting a number in any base left one digit multiplies its

value by the base

‚Ä¢  Shifting a number in any base right one digit divides its value
by the base

‚Ä¢ Examples:

üûê 1010 shift left = 10010   üûê 1010 shift right = 110

üûê  102 shift left = 1002    üûê  102 shift right = 12

Copyright 2010 John Wiley & Sons, Inc.


Binary Multiplication

1  1  0  1

1  0  1

1  1  0  1  1‚Äôs place

0    2‚Äôs place

1  1  0  1       4‚Äôs place (bits shifted to line up with 4‚Äôs place of multiplier)

1  0  0  0  0  0  1  Result (AND)

Copyright 2010 John Wiley & Sons, Inc.


Converting from Base 10

‚Ä¢ Powers Table


Power

Base      8    7    6     5

4     3    2    1    0


2    256

128   64    32    16

8    4    2    1


8                  32,768

4,096

512   64    8    1


16                       65,536

4,096

256   16   1


Copyright 2010 John Wiley & Sons, Inc.

3-30


From Base 10 to Base 2

4210 = 1010102

Power

Base        6     5     4     3     2    1    0


2      64

32    16

8     4    2    1

1     0     1     0    1    0


Integer

Remainder

42/32

= 1

10

10/16

= 0

10

10/8

= 1

2

2/4

= 0

2

2/2

= 1

0

0/1

= 0

0

Copyright 2010 John Wiley & Sons, Inc.


From Base 10 to Base 2

Base 10  42

Remainder


Quotient

2 )  42 ( 0  Least significant bit

2 )  21 ( 1

2 )  10 ( 0

2 )   5 ( 1

2 )   2 ( 0

2 )   1     Most significant bit

Base 2    101010

Copyright 2010 John Wiley & Sons, Inc.


From Base 10 to Base 16

5,73510 = 166716


Power

Base        4        3

2         1      0


16    65,536

4,096

256

16     1


Integer
Remainder

1

5,735 /4,096

= 1

5,735 - 4,096

= 1,639

6

1,639 / 256

= 6

1,639 ‚Äì1,536

= 103

6     7

103 /16    7

= 6

103 ‚Äì 96

= 7

Copyright 2010 John Wiley & Sons, Inc.


From Base 10 to Base 16


Base 10  8,039

Remainder


Quotient

16 )  8,039 ( 7  Least significant bit

16 )   502 ( 6

16 )    31 ( 15

16 )     1 ( 1  Most significant bit

16 )     0

Base 16  1F67

Copyright 2010 John Wiley & Sons, Inc.


From Base 8 to Base 10

72638  = 3,76310

Power     83        82        81        80

512      64      8      1


Sum for

Base 10

x 7

3,584

x 2

128

x 6    x 3

48      3

Copyright 2010 John Wiley & Sons, Inc.


From Base 8 to Base 10

72638  = 3,76310

7

x 8

56  + 2 =  58

x 8

464  + 6 =  470

x 8

3760  + 3 =  3,763

Copyright 2010 John Wiley & Sons, Inc.


From Base 16 to Base 2

‚Ä¢ The nibble approach

‚Ä¢ Hex easier to read and write than binary

Base 16   1     F      6      7


Base 2

0001

1111

0110

0111

‚Ä¢ Why hexadecimal?

‚Ä¢  Modern computer operating systems and networks present
variety of troubleshooting data in hex format

Copyright 2010 John Wiley & Sons, Inc.


Fractions

‚Ä¢ Number point or radix point

‚Ä¢ Decimal point in base 10

‚Ä¢ Binary point in base 2

‚Ä¢ No exact relationship between fractional numbers in different number
bases

‚Ä¢ Exact conversion may be impossible


Copyright 2010 John Wiley & Sons, Inc.

3-38


Decimal Fractions

‚Ä¢ Move the number point one place to the right

‚Ä¢ Effect: multiplies the number by the base number

‚Ä¢ Example: 139.010             139010

‚Ä¢ Move the number point one place to the left

‚Ä¢ Effect: divides the number by the base number

‚Ä¢ Example: 139.010             13.910


Copyright 2010 John Wiley & Sons, Inc.

3-39


Fractions: Base 10 and Base 2

.258910

Place        10-1          10-2            10-3            10-4

Value        1/10       1/100        1/1000       1/10000

Evaluate     2 x 1/10     5 x 1/100     8 x 1/1000     9 x1/1000

Sum          .2         .05         .008         .0009

.101011‚ÇÇ = 0.671875‚ÇÅ‚ÇÄ

Place        2-1       2-2         2-3        2-4       2-5        2-6

Value       1/2     1/4       1/8      1/16     1/32     1/64
Evaluate    1 x 1/2   0 x 1/4    1x 1/8    0 x 1/16  1 x 1/32   1 x 1/64
Sum         .5             0.125           0.03125   0.015625

Copyright 2010 John Wiley & Sons, Inc.


Fractions: Base 10 and Base 2

‚Ä¢ No general relationship between fractions of types
1/10·µè and 1/2·µè

‚Ä¢ Therefore a number representable in base 10 may not be
representable in base 2

‚Ä¢ But: the converse is true: all fractions of the form 1/2·µè

can be represented in base 10

‚Ä¢ Fractional conversions from one base to another are
stopped

‚Ä¢ If there is a rational solution or

‚Ä¢ When the desired accuracy is attained


Copyright 2010 John Wiley & Sons, Inc.

3-41


Mixed Number Conversion

‚Ä¢ Integer and fraction parts must be converted separately

‚Ä¢ Radix point: fixed reference for the conversion

‚Ä¢ Digit to the left is a unit digit in every base

‚Ä¢ B‚Å∞ is always 1 regardless of the base


Copyright 2010 John Wiley & Sons, Inc.

3-42


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work beyond that permitted
in section 117 of the 1976 United States Copyright Act without express permission
of the copyright owner is unlawful. Request for further information should be
addressed to the Permissions Department, John Wiley & Sons, Inc. The purchaser
may make back-up copies for his/her own use only and not for distribution or
resale. The Publisher assumes no responsibility for errors, omissions, or damages
caused by the use of these programs or from the use of the information contained
herein.‚Äù


Copyright 2010 John Wiley & Sons, Inc.

3-43


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 4: Data Formats


Data Formats

‚Ä¢ Computers

‚Ä¢ Process and store all forms of data in binary format

‚Ä¢ Human communication

‚Ä¢ Includes language, images and sounds

‚Ä¢ Data formats:

‚Ä¢ Specifications for converting data into computer-usable form

‚Ä¢ Define the different ways human data may be represented,
stored and processed by a computer


Sources of Data

‚Ä¢ Binary input

‚Ä¢ Begins as discrete input

‚Ä¢ Example: keyboard input such as A 1+2=3 math

‚Ä¢ Keyboard generates a binary number code for each key

‚Ä¢ Analog

‚Ä¢ Continuous data such as sound or images

‚Ä¢ Requires hardware to convert data into binary numbers


A 1+2=3 math

Input
device

Computer

1101000101010101‚Ä¶

Figure 3.1 with this color scheme


Common Data Representations


Type of Data
Alphanumeric
Image (bitmapped)

Image (object)

Outline graphics and fonts
Sound

Page description
Video

Standard(s)

Unicode, ASCII, EDCDIC

‚ñ™GIF (graphical image format)

‚ñ™TIF (tagged image file format)

‚ñ™PNG (portable network graphics)

PostScript, JPEG, SWF (Macromedia Flash),
SVG

PostScript, TrueType

WAV, AVI, MP3, MIDI, WMA

PDF (Adobe Portable Document Format),
HTML, XML

Quicktime, MPEG-2, RealVideo, WMV


Internal Data Representation

‚Ä¢ Reflects the

‚Ä¢ Complexity of input source

‚Ä¢ Type of processing required

‚Ä¢ Trade-offs

‚Ä¢ Accuracy and resolution

‚Ä¢  Simple photo vs. painting in an art book

‚Ä¢ Compactness (storage and transmission)

‚Ä¢  More data required for improved accuracy and resolution

‚Ä¢  Compression represents data in a more compact form

‚Ä¢  Metadata: data that describes or interprets the meaning of data

‚Ä¢ Ease of manipulation:

‚Ä¢  Processing simple audio vs. high-fidelity sound

‚Ä¢ Standardization

‚Ä¢  Proprietary formats for storing and processing data (WordPerfect vs. Word)

‚Ä¢  De facto standards: proprietary standards based on general user acceptance (PostScript)


Data Types: Numeric

‚Ä¢ Used for mathematical manipulation

‚Ä¢ Add, subtract, multiply, divide

‚Ä¢ Types

‚Ä¢ Integer (whole number)

‚Ä¢ Real (contains a decimal point)

‚Ä¢ Covered in Chapters 4 and 5


Data Types: Alphanumeric

‚Ä¢ Alphanumeric:

‚Ä¢ Characters: b T

‚Ä¢ Number digits: 7 9

‚Ä¢ Punctuation marks: ! ;

‚Ä¢ Special-purpose characters: $ &

‚Ä¢ Numeric characters vs. numbers

‚Ä¢ Both entered as ordinary characters

‚Ä¢ Computer converts into numbers for calculation

‚Ä¢  Examples: Variables declared as numbers by the programmer (Salary$ in
BASIC)

‚Ä¢ Treated as characters if processed as text

‚Ä¢  Examples: Phone numbers, ZIP codes


Alphanumeric Codes

‚Ä¢ Arbitrary choice of bits to represent characters

‚Ä¢ Consistency: input and output device must
recognize same code

‚Ä¢ Value of binary number representing character
corresponds to placement in the alphabet

‚Ä¢ Facilitates sorting and searching


Representing Characters

‚Ä¢ ASCII - most widely used coding scheme

‚Ä¢ EBCDIC: IBM mainframe (legacy)

‚Ä¢ Unicode: developed for worldwide use


ASCII

‚Ä¢ Developed by ANSI (American National Standards Institute)

‚Ä¢ Represents

‚Ä¢ Latin alphabet, Arabic numerals, standard punctuation
characters

‚Ä¢ Plus small set of accents and other European special
characters

‚Ä¢ ASCII

‚Ä¢ 7-bit code: 128 characters


ASCII Reference Table


LSD

MSD

0

1

2

3

4

5

6

7

8

9

A
B
C

D
E
F

0

NUL
SOH
STX
ETX
EOT
ENQ
ACJ
BEL
BS
HT
LF
VT
FF

CR
SO
SI

1       2       3

DLE      SP       0

DC1       !        1

DC2       ‚Äú        2

DC3       #        3

DC4       $        4

NAK       %        5

SYN       &        6

ETB       ‚Äò        7

CAN       (        8

EM       )        9

SUB       *        :

ESC       +        ;

FS        ,        <

GS       -        =

RS       .        >

US       /        ?

4       5       6

@       P

A        Q        a

B        R        b

C        S        c

D        T        d

E        U        e

F        V        f

G       W        g

H        X        h

I        Y        i

J        Z        j

K        [        k

L        \        l

M        ]        m

N        ^        n

O        _        o

7

p
W

r
s
t
u
v
w
x
y
z

{

|

}

~
DEL


EBCDIC

‚Ä¢ Extended Binary Coded Decimal Interchange Code
developed by IBM

‚Ä¢ Restricted mainly to IBM or IBM compatible
mainframes

‚Ä¢ Conversion software to/from ASCII available

‚Ä¢ Common in archival data

‚Ä¢ Character codes differ from ASCII


Space
A

b

ASCII
2016

4116

6216

EBCDIC
4016
C116

8216


Unicode

‚Ä¢ Most common 16-bit form represents 65,536 characters

‚Ä¢ ASCII Latin-I subset of Unicode

‚Ä¢ Values 0 to 255 in Unicode table

‚Ä¢ Multilingual: defines codes for

‚Ä¢ Nearly every character-based alphabet

‚Ä¢ Large set of ideographs for Chinese, Japanese and Korean

‚Ä¢ Composite characters for vowels and syllabic clusters required by

some languages

‚Ä¢ Allows software modifications for local-languages


Collating Sequence

‚Ä¢ Alphabetic sorting if software handles mixed upper- and
lowercase codes

‚Ä¢ In ASCII, numbers collate first; in EBCDIC, last

‚Ä¢ ASCII collating sequence for string of characters


Letters

Adam      A  d  a  m

Adamian    A  d  a  m  i  a  n
Adams     A  d  a  m  s

Numeric Characters

1  011  0001

12  011  0001  011  0010

2  011  0010


2 Classes of Codes

‚Ä¢ Printing characters

‚Ä¢ Produced on the screen or printer

‚Ä¢ Control characters

‚Ä¢ Control position of output on screen or printer

üûê VT: vertical tab   üûê LF: Line feed

‚Ä¢ Cause action to occur

üûê BEL: bell rings    üûê DEL: delete current character

‚Ä¢ Communicate status between computer and I/O device

üûê ESC: provides extensions by changing the meaning of a
specified number of contiguous following characters


Keyboard Input

‚Ä¢ Scan code

‚Ä¢ Two different scan codes on keyboard

‚Ä¢  One generated when key is struck and another when key is released

‚Ä¢ Converted to Unicode, ASCII or EBCDIC by software in terminal
or PC

‚Ä¢ Advantage

‚Ä¢ Easily adapted to different languages or keyboard layout

‚Ä¢ Separate scan codes for key press/release for multiple key
combinations

‚Ä¢  Examples: shift and control keys


Other Alphanumeric Input

‚Ä¢ OCR (optical character reader)

‚Ä¢ Scans text and inputs it as character data

‚Ä¢ Used to read specially encoded characters

‚Ä¢  Example: magnetically printed check numbers

‚Ä¢ Bar Code Readers

‚Ä¢  Used in applications that require fast, accurate and repetitive input with minimal
employee training

‚Ä¢ Examples: supermarket checkout counters and inventory control

‚Ä¢ Magnetic stripe reader: alphanumeric data from credit cards

‚Ä¢ RFID: store and transmit data between RFID tags and computers

‚Ä¢ Voice

‚Ä¢ Digitized audio recording common but conversion to alphanumeric data difficult

‚Ä¢ Requires knowledge of sound patterns in a language (phonemes) plus rules for

pronunciation, grammar, and syntax


Image Data

‚Ä¢ Photographs, figures, icons, drawings, charts and graphs

‚Ä¢ Two approaches:

‚Ä¢ Bitmap or raster images of photos and paintings with continuous variation

‚Ä¢ Object or vector images composed of graphical objects like lines and curves

defined geometrically

‚Ä¢ Differences include:

‚Ä¢ Quality of the image

‚Ä¢ Storage space required

‚Ä¢ Time to transmit

‚Ä¢ Ease of modification


Bitmap Images

‚Ä¢ Used for realistic images with continuous variations in shading, color,
shape and texture

‚Ä¢ Examples:

‚Ä¢  Scanned photos

‚Ä¢  Clip art generated by a paint program

‚Ä¢ Preferred when image contains large amount of detail and processing
requirements are fairly simple

‚Ä¢ Input devices:

‚Ä¢ Scanners

‚Ä¢ Digital cameras and video capture devices

‚Ä¢ Graphical input devices like mice and pens

‚Ä¢ Managed by photo editing software or paint software

‚Ä¢ Editing tools to make tedious bit by bit process easier


Bitmap Images

‚Ä¢ Each individual pixel (pi(x)cture element) in a graphic
stored as a binary number

‚Ä¢ Pixel: A small area with associated coordinate location

‚Ä¢ Example: each point below represented by a 4-bit code
corresponding to 1 of 16 shades of gray


Bitmap Display

‚Ä¢ Monochrome: black or white

‚Ä¢ 1 bit per pixel

‚Ä¢ Gray scale: black, white or 254 shades of gray

‚Ä¢ 1 byte per pixel

‚Ä¢ Color graphics: 16 colors, 256 colors, or 24-bit true color
(16.7 million colors)

‚Ä¢ 4, 8, and 24 bits respectively


Storing Bitmap Images

‚Ä¢ Frequently large files

‚Ä¢ Example: 600 rows of 800 pixels with 1 byte for each of 3
colors   ~1.5MB file

‚Ä¢ File size affected by

‚Ä¢ Resolution (the number of pixels per inch)

‚Ä¢  Amount of detail affecting clarity and sharpness of an image

‚Ä¢ Levels: number of bits for displaying shades of gray or
multiple colors

‚Ä¢  Palette: color translation table that uses a code for each pixel rather
than actual color value

‚Ä¢ Data compression


GIF (Graphics Interchange Format)

‚Ä¢ First developed by CompuServe in 1987

‚Ä¢ GIF89a enabled animated images

‚Ä¢ allows images to be displayed sequentially at fixed time sequences

‚Ä¢ Color limitation: 256

‚Ä¢ Image compressed by LZW (Lempel-Zif-Welch) algorithm

‚Ä¢ Preferred for line drawings, clip art and pictures with large
blocks of solid color

‚Ä¢ Lossless compression


GIF (Graphics Interchange Format)


JPEG

(Joint Photographers Expert Group)

‚Ä¢ Allows more than 16 million colors

‚Ä¢ Suitable for highly detailed photographs and paintings

‚Ä¢ Employs lossy compression algorithm that

‚Ä¢ Discards data to decreases file size and transmission speed

‚Ä¢ May reduce image resolution, tends to distort sharp lines


Object Images

‚Ä¢ Created by drawing packages or output from spreadsheet data
graphs

‚Ä¢ Composed of lines and shapes in various colors

‚Ä¢ Computer translates geometric formulas to create the graphic

‚Ä¢ Storage space depends on image complexity

‚Ä¢ number of instructions to create lines, shapes, fill patterns

‚Ä¢ Movies Shrek and Toy Story use object images


Object Images

‚Ä¢ Based on mathematical formulas

‚Ä¢ Easy to move, scale and rotate without losing shape and
identity as bitmap images may

‚Ä¢ Require less storage space than bitmap images

‚Ä¢ Cannot represent photos or paintings

‚Ä¢ Cannot be displayed or printed directly

‚Ä¢ Must be converted to bitmap since output devices except
plotters are bitmap


PostScript

‚Ä¢ Page description language: list of procedures and statements
that describe each of the objects to be printed on a page

‚Ä¢ Stored in ASCII or Unicode text file

‚Ä¢ Interpreter program in computer or output device reads
PostScript to generate image

‚Ä¢ Scalable font support

‚Ä¢ Font outline objects specified like other objects


Bitmap vs. Object Images

Bitmap (Raster)                 Object (Vector)

Pixel map                     Geometrically defined shapes

Photographic quality              Complex drawings

Paint software                  Drawing software

Larger storage requirements         Higher computational requirements


Enlarging images produces jagged
edges

Resolution of output limited by

resolution of image

Objects scale smoothly

Resolution of output limited by output

device


Video Images

‚Ä¢ Require massive amount of data

‚Ä¢  Video camera producing full screen 640 x 480 pixel true color image at 30
frames/sec           27.65 MB of data/sec

‚Ä¢  1-minute film clip      1.6 GB storage

‚Ä¢ Options for reducing file size: decrease size of image, limit number of
colors, reduce frame rate

‚Ä¢ Method depends on how video delivered to users

‚Ä¢ Streaming video: video displayed as it is downloaded from the Web server

‚Ä¢ Local data (file on DVD or downloaded onto system) for higher quality

‚Ä¢  MPEG-2: movie quality images with high compression require substantial processing

capability


Audio Data

‚Ä¢ Transmission and processing requirements less
demanding than those for video

‚Ä¢ Waveform audio: digital representation of sound

‚Ä¢ MIDI (Musical Instrument Digital Interface): instructions
to recreate or synthesize sounds

‚Ä¢ Analog sound converted to digital values by A-to-D
converter


Waveform Audio

Sampling rate

normally 50KHz


Sampling Rate

‚Ä¢ Number of times per second that sound is measured during
the recording process.

‚Ä¢ 1000 samples per second = 1 KHz (kilohertz)

‚Ä¢ Example: Audio CD sampling rate = 44.1KHz

‚Ä¢ Height of each sample saved as:

‚Ä¢ 8-bit number for radio-quality recordings

‚Ä¢ 16-bit number for high-fidelity recordings

‚Ä¢ 2 x 16-bits for stereo


Audio Formats

‚Ä¢ MP3

‚Ä¢ Derivative of MPEG-2 (ISO Moving Picture Experts Group)

‚Ä¢ Uses psychoacoustic compression techniques to reduce
storage requirements

‚Ä¢ WAV

‚Ä¢ Developed by Microsoft as part of its multimedia specification

‚Ä¢ General-purpose format for storing and reproducing small
snippets of sound


Audio Data Formats


Data Compression

‚Ä¢ Compression: recoding data so that it requires fewer bytes of storage
space.

‚Ä¢ Compression ratio: the amount file is shrunk

‚Ä¢ Lossless: inverse algorithm restores data to exact original form

‚Ä¢ Examples: GIF, PCX, TIFF

‚Ä¢ Lossy: trades off data degradation for file size and download speed

‚Ä¢ Much higher compression ratios, often 10 to 1

‚Ä¢ Example: JPEG

‚Ä¢ Common in multimedia

‚Ä¢ MPEG-2: uses both forms for ratios of 100:1


Page Description Languages

‚Ä¢ Describe layout of objects on a displayed or printed page

‚Ä¢ Objects may include text, object images, bitmap images,
multimedia objects, and other data formats

‚Ä¢ Examples

‚Ä¢ HTML, XHTML, XML

‚Ä¢ PDF

‚Ä¢ Postscript


Internal Computer Data Format

‚Ä¢ All data stored as binary numbers

‚Ä¢ Interpreted based on

‚Ä¢ Operations computer can perform

‚Ä¢ Data types supported by programming language used
to create application


5 Simple Data Types

‚Ä¢ Boolean: 2-valued variables or constants with values of true or false

‚Ä¢ Char: Variable or constant that holds alphanumeric character

‚Ä¢ Enumerated

‚Ä¢ User-defined data types with possible values listed in definition

‚Ä¢  Type DayOfWeek = Mon, Tues, Wed, Thurs, Fri, Sat, Sun

‚Ä¢ Integer: positive or negative whole numbers

‚Ä¢ Real

‚Ä¢ Numbers with a decimal point

‚Ä¢ Numbers whose magnitude, large or small, exceeds computer‚Äôs capability to store as

an integer


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work
beyond that permitted in section 117 of the 1976 United
States Copyright Act without express permission of the
copyright owner is unlawful. Request for further information
should be addressed to the Permissions Department, John
Wiley & Sons, Inc. The purchaser may make back-up copies
for his/her own use only and not for distribution or resale.

The Publisher assumes no responsibility for errors,

omissions, or damages caused by the use of these programs
or from the use of the information contained herein.‚Äù


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 5:

Representing Numerical Data

The Architecture of Computer Hardware and Systems
Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John                  Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong, Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-authored with Lynne Senne,
Bentley University


Number Representation

‚Ä¢ Numbers can be represented as a combination of

‚Ä¢ Value or magnitude

‚Ä¢ Sign (plus or minus)

‚Ä¢ Decimal (if necessary)


Copyright 2010 John Wiley & Sons, Inc

5-5


Unsigned Numbers: Integers

‚Ä¢ Unsigned whole number or integer

‚Ä¢ Direct binary equivalent of decimal integer

‚ñ™ 4 bits: 0 to 9      ‚ñ™ 16 bits: 0 to 9,999

‚ñ™ 8 bits: 0 to 99      ‚ñ™ 32 bits: 0 to 99,999,999

Decimal        Binary                BCD

68      = 0100 0100            = 0110        1000

= 26 + 22 = 64 + 4 = 68     = 22 + 21 = 6    23 = 8


99

(largest 8-bit

BCD)

255

(largest 8-bit
binary)

Copyright 2010 John Wiley & Sons, Inc

= 0110 0011

= 26 + 25 + 21 + 20 =

= 64 + 32 + 2 + 1 = 99

= 1111 1111

= 28 ‚Äì 1 = 255

= 1001       1001

= 23 + 20       23 + 20

=   9         9

= 0010  0101  0101

= 21   22 + 20   22 + 20

= 2     5     5


Value Range: Binary vs. BCD

‚Ä¢ BCD range of values < conventional binary representation

‚Ä¢ Binary: 4 bits can hold 16 different values (0 to 15)

‚Ä¢ BCD: 4 bits can hold only 10 different values (0 to 9)


No. of Bits

4

8

12

16

20

24

32

64

BCD Range

0-9           1 digit

0-99          2 digits

0-999         3 digits

0-9,999        4 digits

0-99,999       5 digits

0-999,999      6 digits

0-99,999,999    8 digits

0-(1016-1)       16 digits

Binary Range

0-15          1+ digit

0-255         2+ digits

0-4,095        3+ digits

0-65,535       4+ digits

0-1 million      6 digits

0-16 million      7+ digits

0-4 billion       9+ digits

0-16 quintillion    19+ digits

Copyright 2010 John Wiley & Sons, Inc


Conventional Binary vs. BCD

‚Ä¢ Binary representation generally preferred

‚Ä¢ Greater range of value for given number of bits

‚Ä¢ Calculations easier

‚Ä¢ BCD often used in business applications to maintain decimal rounding
and decimal precision


Copyright 2010 John Wiley & Sons, Inc

5-8


Simple BCD Multiplication


Copyright 2010 John Wiley & Sons, Inc

5-9


Packed Decimal Format

‚Ä¢ Real numbers representing dollars and cents

‚Ä¢ Support by business-oriented languages like COBOL

‚Ä¢ IBM System 370/390 and Compaq Alpha

Copyright 2010 John Wiley & Sons, Inc


Signed-Integer Representation

‚Ä¢ No obvious direct way to represent the sign in binary notation

‚Ä¢ Options:

‚Ä¢ Sign-and-magnitude representation

‚Ä¢ 1‚Äôs complement

‚Ä¢ 2‚Äôs complement (most common)


Copyright 2010 John Wiley & Sons, Inc

5-11


Sign-and-Magnitude

‚Ä¢ Use left-most bit for sign

‚Ä¢ 0 = plus; 1 = minus

‚Ä¢ Total range of integers the same

‚Ä¢ Half of integers positive; half negative

‚Ä¢ Magnitude of largest integer half as large

‚Ä¢ Example using 8 bits:

‚Ä¢ Unsigned: 1111 1111 = +255

‚Ä¢ Signed:     0111 1111 = +127

1111 1111 = -127

‚Ä¢ Note: 2 values for 0:

+0 (0000 0000) and -0 (1000 0000)


Copyright 2010 John Wiley & Sons, Inc

5-12


Difficult Calculation Algorithms

‚Ä¢ Sign-and-magnitude algorithms complex and difficult to

implement in hardware

‚Ä¢ Must test for 2 values of 0

‚Ä¢ Useful with BCD

‚Ä¢ Order of signed number and carry/borrow makes a difference

‚Ä¢ Example: Decimal addition algorithm


Addition:

2 Positive Numbers

Addition:

1 Signed Number


4

+2

6

Copyright 2010 John Wiley & Sons, Inc

4     2     12

- 2    - 4     - 4

2     -2      8


9‚Äôs Decimal Complement

‚Ä¢ Taking the complement: subtracting a value from a standard basis

value

‚Ä¢ Decimal (base 10) system diminished radix complement

‚Ä¢  Radix minus 1 = 10 ‚Äì 1    9 as the basis

‚Ä¢ 3-digit example: base value = 999

‚Ä¢  Range of possible values 0 to 999 arbitrarily split at 500


Numbers
Representation method

Range of decimal numbers
Calculation
Representation example

999 ‚Äì 499

Negative
Complement

-499    -000

999 minus number

500    999

‚Äì    Increasing

Positive
Number itself

+0    499

none

0    499

value    +

Copyright 2010 John Wiley & Sons, Inc


Complementary Representation

‚Ä¢ Sign of the number does not have to be handled separately

‚Ä¢ Consistent for all different signed combinations of input numbers

‚Ä¢ Two methods

‚Ä¢ Radix: value used is the base number

‚Ä¢ Diminished radix: value used is the base number minus 1

‚Ä¢  9‚Äôs complement: base 10 diminished radix

‚Ä¢  1‚Äôs complement: base 2 diminished radix


Copyright 2010 John Wiley & Sons, Inc

5-15


9‚Äôs Decimal Complement

‚Ä¢ Necessary to specify number of digits or word size

‚Ä¢ Example: representation of 3-digit number

‚Ä¢ First digit = 0 through 4  positive number

‚Ä¢ First digit = 5 through 9  negative number

‚Ä¢ Conversion to sign-and-magnitude number for 9‚Äôs complement

‚Ä¢ 321 remains 321

‚Ä¢ 521: take the complement (999 ‚Äì 521) = ‚Äì 478


Copyright 2010 John Wiley & Sons, Inc

5-16


Choice of Representation

‚Ä¢ Must be consistent with rules of normal arithmetic

‚Ä¢ - (-value) = value

‚Ä¢ If we complement the value twice, it should return to its original value

‚Ä¢ Complement = basis ‚Äì value

‚Ä¢ Complement twice

‚Ä¢ Basis ‚Äì (basis ‚Äì value) = value


Copyright 2010 John Wiley & Sons, Inc

5-17


Modular Addition

‚Ä¢ Counting upward on scale corresponds to addition

‚Ä¢ Example in 9‚Äôs complement: does not cross the modulus


+250

Representation   500  649       899  999

+250

0   170       420  499


Number

represented

-499  -350      -100  -000

+250

0   170       420  499

+250

Copyright 2010 John Wiley & Sons, Inc


Addition with Wraparound

‚Ä¢ Count to the right to add a negative number

‚Ä¢ Wraparound scale used to extend the range for the negative
result

‚Ä¢  Counting left would cross the modulus and give incorrect answer
because there are 2 values for 0 (+0 and -0)

+699

Representation   500  999   0   200   499        500  899  999


Number

represented

-499 -000

0  200  499

-300

-499 -100 -000

Wrong Answer!!                          +699

Representation          500    898    999         0     200     499


Number
represented

Copyright 2010 John Wiley & Sons, Inc

-499   -101   -000

- 300

0    200    499


Addition with End-around Carry

‚Ä¢ Count to the right crosses the modulus

‚Ä¢ End-around carry

‚Ä¢ Add 2 numbers in 9‚Äôs complementary arithmetic

‚Ä¢ If the result has more digits than specified, add carry to
the result


+300

(1099)         799


Representation  500  799  999

0    99    499

300


Number
represented

-499 -200 -000

+300

0   100    499

1099

1

100

Copyright 2010 John Wiley & Sons, Inc


Overflow

‚Ä¢ Fixed word size has a fixed range size

‚Ä¢ Overflow: combination of numbers that adds to result outside the
range

‚Ä¢ End-around carry in modular arithmetic avoids problem

‚Ä¢ Complementary arithmetic: numbers out of range have the opposite
sign

‚Ä¢ Test: If both inputs to an addition have the same sign and the output sign is
different, an overflow occurred


Copyright 2010 John Wiley & Sons, Inc

5-21


1‚Äôs Binary Complement

‚Ä¢ Taking the complement: subtracting a value from a standard basis value

‚Ä¢  Binary (base 2) system diminished radix complement

‚Ä¢  Radix minus 1 = 2 ‚Äì 1   1 as the basis

‚Ä¢ Inversion: change 1‚Äôs to 0‚Äôs and 0‚Äôs to 1s

‚Ä¢  Numbers beginning with 0 are positive

‚Ä¢  Numbers beginning with 1 are negative

‚Ä¢  2 values for zero

‚Ä¢ Example with 8-bit binary numbers


Numbers
Representation method

Range of decimal numbers
Calculation
Representation example

Negative
Complement

-12710     -010

Inversion

10000000 11111111

Positive
Number itself

+010      12710

None

00000000  01111111

Copyright 2010 John Wiley & Sons, Inc


Conversion between Complementary Forms

‚Ä¢ Cannot convert directly between 9‚Äôs complement and 1‚Äôs complement

‚Ä¢ Modulus in 3-digit decimal: 999

‚Ä¢ Positive range 499

‚Ä¢ Modulus in 8-bit binary:
11111111 or 255‚ÇÅ‚ÇÄ

‚Ä¢ Positive range 01111111 or 12710

‚Ä¢ Intermediate step: sign-and-magnitude representation


Copyright 2010 John Wiley & Sons, Inc

5-23


Addition

‚Ä¢ Add 2 positive 8-bit numbers

‚Ä¢ Add 2 8-bit numbers with
different signs

‚Ä¢ Take the 1‚Äôs complement of  58
(i.e., invert)

0011 1010

1100 0101

0010 1101 =   45

0011 1010 =   58

0110 0111 =  103

0010 1101 =   45

1100 0101 =  ‚Äì58

1111 0010 =  ‚Äì13


Invert to get
magnitude

0000 1101

8 + 4 + 1 =  13

Copyright 2010 John Wiley & Sons, Inc


Addition with Carry

‚Ä¢ 8-bit number

‚Ä¢ Invert

0000 0010 (2‚ÇÅ‚ÇÄ)

1111 1101

‚Ä¢ Add

‚Ä¢ 9 bits

End-around carry

0110 1010 =   106

1111 1101 =    ‚Äì2

10110 0111

+1

0110 1000 =   104

Copyright 2010 John Wiley & Sons, Inc


Subtraction

‚Ä¢ 8-bit number

‚Ä¢ Invert

0101 1010 (90‚ÇÅ‚ÇÄ)

1010 0101

‚Ä¢ Add

‚Ä¢ 9 bits

End-around carry

0110 1010 =   106

-0101 1010 =    90

0110 1010 =   106

‚Äì1010 0101 =    90

10000 1111

+1

0001 0000 =    16

Copyright 2010 John Wiley & Sons, Inc


Overflow

‚Ä¢ 8-bit number

‚Ä¢ 256 different numbers

‚Ä¢ Positive numbers:
0 to 127

‚Ä¢ Add

‚Ä¢ Test for overflow

0100 0000 =   64

0100 0001 =   65

1000 0001   -126

0111 1110


‚Ä¢ 2 positive inputs
produced negative
result   overflow!

‚Ä¢ Wrong answer!

Invert to get
magnitude

12610

‚Ä¢ Programmers beware: some high-level languages,
e.g., some versions of BASIC, do not check for
overflow adequately

Copyright 2010 John Wiley & Sons, Inc


10‚Äôs Complement

‚Ä¢ Create complementary system with a single 0

‚Ä¢ Radix complement: use the base for complementary operations

‚Ä¢ Decimal base: 10‚Äôs complement

‚Ä¢ Example: Modulus 1000 as the as reflection point


Numbers
Representation method

Range of decimal numbers
Calculation
Representation example

Negative
Complement

-500    -001

1000 minus number

500     999

Positive
Number itself

0    499

none

0   499

Copyright 2010 John Wiley & Sons, Inc


Examples with 3-Digit Numbers

‚Ä¢ Example 1:

‚Ä¢ 10‚Äôs complement representation of 247

‚Ä¢  247 (positive number)

‚Ä¢ 10‚Äôs complement of 227

‚Ä¢  1000 ‚Äì 247 = 753 (negative number)

‚Ä¢ Example 2:

‚Ä¢ 10‚Äôs complement of 17

‚Ä¢  1000 ‚Äì 017 = 983

‚Ä¢ Example 3:

‚Ä¢ 10‚Äôs complement of 777

‚Ä¢  Negative number because first digit is 7

‚Ä¢  1000 ‚Äì 777 = 223

‚Ä¢  Signed value = -223

Copyright 2010 John Wiley & Sons, Inc


Alternative Method
for 10‚Äôs Complement

‚Ä¢ Based on 9‚Äôs complement

‚Ä¢ Example using 3-digit number

‚Ä¢ Note: 1000 = 999 + 1

‚Ä¢ 9‚Äôs complement = 999 ‚Äì value

‚Ä¢ Rewriting

‚Ä¢  10‚Äôs complement = 1000 ‚Äì value = 999 + 1 ‚Äì value

‚Ä¢ Or: 10‚Äôs complement = 9‚Äôs complement + 1

‚Ä¢ Computationally easier especially when working with binary
numbers


Copyright 2010 John Wiley & Sons, Inc

5-8


2‚Äôs Complement

‚Ä¢ Modulus = a base 2 ‚Äú1‚Äù followed by specified number of 0‚Äôs

‚Ä¢ For 8 bits, the modulus = 1000 0000

‚Ä¢ Two ways to find the complement

‚Ä¢ Subtract value from the modulus or invert


Numbers

Representation method

Range of decimal
numbers

Calculation

Representation
example

Copyright 2010 John Wiley & Sons, Inc

Negative
Complement

-12810      -110

Inversion

10000000  11111111

Positive
Number itself

+010      12710

None

00000000  01111111


Estimating Integer Size

‚Ä¢ Positive numbers begin with 0

‚Ä¢ Small negative numbers (close to 0) begin with multiple 0‚Äôs

‚Ä¢ 1111 1110 = -2 in 8-bit 2‚Äôs complements

‚Ä¢ 1000 0000 = -128, largest negative 2‚Äôs complements

‚Ä¢ Invert all 1‚Äôs and 0‚Äôs and approximate the value


Copyright 2010 John Wiley & Sons, Inc

5-10


Overflow and Carry Conditions

‚Ä¢ Carry flag: set when the result of an addition or subtraction exceeds
fixed number of bits allocated

‚Ä¢ Overflow: result of addition or subtraction overflows into the sign bit


Copyright 2010 John Wiley & Sons, Inc

5-11


Exponential Notation

‚Ä¢ Also called scientific notation

‚ñ™ 12345        ‚ñ™ 12345 x 100

‚ñ™ 0.12345 x 105   ‚ñ™ 123450000 x 10-4

‚Ä¢  4 specifications required for a number

1.  Sign (‚Äú+‚Äù in example)

2.  Magnitude or mantissa (12345)

3.  Sign of the exponent (‚Äú+‚Äù in 10‚Åµ)

4.  Magnitude of the exponent (5)

‚Ä¢ Plus

5.  Base of the exponent (10)

6.  Location of decimal point (or other base) radix point


Copyright 2010 John Wiley & Sons, Inc

5-12


Summary of Rules

Sign of the mantissa  Sign of the exponent

-0.35790 x 10-6


Location
of decimal
point

Mantissa   Base   Exponent

Copyright 2010 John Wiley & Sons, Inc


Format Specification

‚Ä¢ Predefined format, usually in 8 bits

‚Ä¢ Increased range of values (two digits of exponent) traded for
decreased precision (two digits of mantissa)

Sign of the mantissa

SEEMMMMM

2-digit Exponent    5-digit Mantissa


Copyright 2010 John Wiley & Sons, Inc

5-14


Format

‚Ä¢ Mantissa: sign digit in sign-magnitude format

‚Ä¢ Assume decimal point located at beginning of
mantissa

‚Ä¢ Excess-N notation: Complementary notation

‚Ä¢ Pick middle value as offset where N is the
middle value


Representation

Exponent being represented

0

-50

49    50    99

-1      0     49

‚Äì   Increasing value      +

Copyright 2010 John Wiley & Sons, Inc


Overflow and Underflow

‚Ä¢ Possible for the number to be too large or too
small for representation

Copyright 2010 John Wiley & Sons, Inc


Floating Point Calculations

‚Ä¢ Addition and subtraction

‚Ä¢ Exponent and mantissa treated separately

‚Ä¢ Exponents of numbers must agree

‚Ä¢ Align decimal points

‚Ä¢ Least significant digits may be lost

‚Ä¢ Mantissa overflow requires exponent again shifted right


Copyright 2010 John Wiley & Sons, Inc

5-17


Addition and Subtraction

Add 2 floating point numbers          05199520

+ 04967850

Align exponents                   05199520

0510067850

Add mantissas; (1) indicates a carry       (1)0019850

Carry requires right shift             05210019(850)

Round                          05210020

Check results

05199520 = 0.99520 x 101  =  9.9520

04967850 = 0.67850 x 10-1  =  0.06785

= 10.01985

In exponential form               =  0.1001985 x 102

Copyright 2010 John Wiley & Sons, Inc


Multiplication and Division

‚Ä¢ Mantissas: multiplied or divided

‚Ä¢ Exponents: added or subtracted

‚Ä¢ Normalization necessary to

‚Ä¢ Restore location of decimal point

‚Ä¢ Maintain precision of the result

‚Ä¢ Adjust excess value since added twice

‚Ä¢ Example: 2 numbers with exponent = 3 represented in excess-50
notation

‚Ä¢ 53 + 53 =106

‚Ä¢ Since 50 added twice, subtract: 106 ‚Äì 50 =56


Copyright 2010 John Wiley & Sons, Inc

5-19


Multiplication and Division

‚Ä¢ Maintaining precision:

‚Ä¢ Normalizing and rounding multiplication

05220000


üûé  Multiply 2 numbers

x  04712500

üûé  Add exponents, subtract offset     52 + 47 ‚Äì 50 = 49

üûé  Multiply mantissas           0.20000 x 0.12500 = 0.025000000

üûé  Normalize the results           04825000

üûé  Round                     05210020

üûé  Check results

05220000 = 0.20000 x 102

04712500 = 0.125 x 10-3

= 0.0250000000 x 10-1

üûé  Normalizing and rounding    =  0.25000 x 10-2

Copyright 2010 John Wiley & Sons, Inc


Floating Point in the Computer

‚Ä¢ Typical floating point format

‚Ä¢ 32 bits provide range ~10‚Åª¬≥‚Å∏ to 10‚Å∫¬≥‚Å∏

‚Ä¢ 8-bit exponent = 256 levels

‚Ä¢  Excess-128 notation

‚Ä¢ 23/24 bits of mantissa: approximately 7 decimal digits of

precision

Copyright 2010 John Wiley & Sons, Inc


IEEE 754 Standard

‚Ä¢ 32-bit Floating Point Value Definition

Exponent    Mantissa         Value

0         ¬±0              0

0         Not 0            ¬±2-126 x 0.M

1-254          Any             ¬±2-127 x 1.M

255        ¬±0              ¬±‚àû

255        not 0            special condition

Copyright 2010 John Wiley & Sons, Inc


Conversion: Base 10 and Base 2

‚Ä¢ Two steps

‚Ä¢ Whole and fractional parts of numbers with an embedded decimal
or binary point must be converted separately

‚Ä¢ Numbers in exponential form must be reduced to a pure decimal
or binary mixed number or fraction before the conversion can be
performed


Copyright 2010 John Wiley & Sons, Inc

5-23


Conversion: Base 10 and Base 2

‚Ä¢ Convert 253.75‚ÇÅ‚ÇÄ to binary floating point form

‚ñ™ Multiply number by 100 25375


‚ñ™ Convert to binary
equivalent

110 0011 0001 1111 or

1.1000 1100 0111 11 x 214

‚ñ™ IEEE Representation   0 10001101 10001100011111


Sign

Excess-127
Exponent = 127 + 14

Mantissa

‚ñ™ Divide by binary floating point equivalent of 100‚ÇÅ‚ÇÄ to
restore original decimal value

Copyright 2010 John Wiley & Sons, Inc


Programming Considerations

‚Ä¢ Integer advantages

‚Ä¢ Easier for computer to perform

‚Ä¢ Potential for higher precision

‚Ä¢ Faster to execute

‚Ä¢ Fewer storage locations to save time and space

‚Ä¢ Most high-level languages provide 2 or more
formats

‚Ä¢ Short integer (16 bits)

‚Ä¢ Long integer (64 bits)


Copyright 2010 John Wiley & Sons, Inc

5-25


Programming Considerations

‚Ä¢ Real numbers

‚Ä¢ Variable or constant has fractional part

‚Ä¢ Numbers take on very large or very small
values outside integer range

‚Ä¢ Program should use least precision
sufficient for the task

‚Ä¢ Packed decimal attractive alternative for
business applications


Copyright 2010 John Wiley & Sons, Inc

5-26


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work beyond that permitted
in section 117 of the 1976 United States Copyright Act without express permission
of the copyright owner is unlawful. Request for further information should be
addressed to the Permissions Department, John Wiley & Sons, Inc. The purchaser
may make back-up copies for his/her own use only and not for distribution or
resale. The Publisher assumes no responsibility for errors, omissions, or damages
caused by the use of these programs or from the use of the information contained
herein.‚Äù


Copyright 2010 John Wiley & Sons, Inc

5-27


Thank You



College of Computing and Informatics
Computer Organization


Assembly Language


Assembly Language

‚Ä¢ Generations of programming languages

‚Ä¢ First generation: programmed directly in binary using wires or
switches.

Image credit: http://professornerdster.com/wp-content/uploads/2012/04/altair_8800.jpeg


Assembly Language

‚Ä¢ Generations of programming languages

‚Ä¢ Second generation: assembly language. Human readable,
converted directly to machine code.

Image credit: http://www.coprolite.com/zhb/pooldog/grnscn.jpg


Assembly Language

‚Ä¢ Generations of programming languages

‚Ä¢ Third generation: high-level languages, while loops, if-then-else,
structured. Most programming today, including object-oriented.

Image credit: http://www.sunspotworld.com/Tutorial/pngs/swemul06.png


Assembly Language

‚Ä¢ Generations of programming languages

‚Ä¢ Fourth generation: 1990s natural languages, non-procedural,
report generation. Use programs to generate other programs.
Limited use today.

Image credit: 
http://docs.oracle.com/cd/E13167_01/aldsp/docs25/appdev/wwimages/CrystalReportsStep4.gif


Assembly Language

‚Ä¢ Generations of programming languages

‚Ä¢ Key idea: Regardless of the language of writing, computers only
process machine code.

‚Ä¢ All non-machine code goes through a translation phase into
machine code.

‚Ä¢ Code generators

‚Ä¢ Compilers

‚Ä¢ Assemblers


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ High level languages use comparison constructs, loops, variables, etc.

‚Ä¢ Machine code is binary, directly executed by CPU.

How does this‚Ä¶            ‚Ä¶become this?


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ Convert high level language to if/goto.


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ Convert high level language
to if/goto.

Use labels for

branch targets.

i = 0

j = 1

k = 0

loop: if (k ‚Äì 10 == 0) goto done

fib = i + j

i = j

j = fib
print i
k   = k + 1
goto loop

done: halt


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ Convert if/goto to assembly
(LMC here).

loop: LDA  k   ; if (k - 10 == 0) goto done

SUB ten ;

BRZ done ;

LDA  i   ; fib = i + j
ADD  j   ;

STO fib ;

LDA j  ; i = j

STO i  ;

LDA fib ; j = fib
STO j  ;

LDA i  ; print i

OUT    ;

LDA k  ; k = k + 1

ADD one ;
STO k  ;

BR  loop ; goto loop
done: HLT    ; halt


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ Convert if/goto to assembly
(LMC here).

loop: LDA  k   ; if (k - 10 == 0) goto done
SUB  ten  ;

BRZ done ;

LDA i  ; fib = i + j

ADD j  ;

STO fib ;
L;DAdatja sec;tiion= j


SjT:O i

DAT ;0   ; i = 0

LiD:A fiDbAT ;1 j = f;ibj = 1


SkT:O j
LfDiAb: i

DAT ;0   ; k = 0

DAT ;0 print; i

OtUeTn: DAT ;10  ;


LoDnAe: k

DAT ;1 k = k; + 1

ADD one ;
STO k  ;

BR  loop ; goto loop

done: HLT    ; halt


Assembly Language

‚Ä¢ Language translation process

‚Ä¢ Assemble the instructions to machine code.


Assembly Language

‚Ä¢ Language translation process

Box

01

02

03

04

Code

520

222

717

519

SUB ten

Assembler

LDA k

BRZ done

LDA i


‚Ä¢ Assemble the

loop: LDA k  ; if (0k5

SUB ten ;

- 10 ==1109)

goto done

ADD j


instructions to

BRZ done ;  06

321

STO fib


machine code.

LDA i  ; fib 0=7

i + j

519

LDA j


ADD j  ;   08

STO fib ;

LDA j     09

319

521

STO i

LDA fib


STO i  ;   10

LDA fib ; j = 1f1ib

STO j  ;

319

519

STO j

LDA i


LDA i

12

; print i

902

OUT


OUT    ;   13

LDA k  ; k = 1k4 + 1

520

123

LDA k

ADD one


ADD one ;   15

STO k  ;

BR  loop ; goto16loop
done: HLT    ; halt17

320

601

000

STO k

BR loop

HLT


Assembly Language

‚Ä¢ Language translation process

Box

01

02

03

04

Code

520

222

717

519

SUB ten

Assembler

LDA k

BRZ done

LDA i


‚Ä¢ Assemble the
instructions to

loop: LDA k  ; if (k

SUB ten ;  05

BRZ done ;  06

- 10 == 0)

119

321

goto done

ADD j

STO fib


machine code.

LDA i  ; fib =

07

i + j

519

LDA j


ADD j  ;

STO fib ;  08

LDA j  ; i = 0j9

319

521

STO i

LDA fib


j is in boSxTO18 i  ;   10

319

STO j

LDA fib ; j = fib


i is in boSxTO19 j  ;   11

519

LDA i


k is in bLoDxA20i  ; prin1t2 i

902

OUT


fib is in ObUoTx 21   ;   13

520

LDA k


ten is inLbDAox 2k2

; k = k + 1


one is inADbDoxo2n3e ;

123

320

ADD one

STO k


BR  loop ; goto16loop
done: HLT    ; halt

17

601

000

BR loop

HLT


Assembly Language

‚Ä¢ Summary

‚Ä¢ High level languages are convenient to read and write for humans.

‚Ä¢ Computers execute only binary machine code.

‚Ä¢ Conversion between the two is required.

‚Ä¢ Compilers translate high level languages to machine code.

‚Ä¢ Assemblers translate assembly language into machine code.

‚Ä¢ Use if/goto pseudo-code as an intermediate language between
high level and assembler.


References

‚Ä¢ Englander, I. (2009). The architecture of computer hardware and
systems software: an information technology approach. Wiley.


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 6:

The Little Man Computer

The Architecture of Computer Hardware, Systems Software
& Networking:

An Information Technology Approach

4th Edition, Irv Englander
John                  Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong, Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-authored with Lynne Senne,
Bentley University


The Little Man Computer


Copyright 2010 John Wiley &
Sons, Inc.

6-5


Mailboxes: Address vs. Content

‚Ä¢ Addresses are consecutive starting at 00 and
ending at 99

‚Ä¢ Content may be

‚Ä¢ Data, a three digit number, or

‚Ä¢ Instructions

Address   Content

Copyright 2010 John Wiley & Sons, Inc.                                        6-6


Content: Instructions

‚Ä¢ Op code

‚Ä¢ In LMC, represented by a single digit

‚Ä¢ Operation code

‚Ä¢ Arbitrary mnemonic

‚Ä¢ Operand

‚Ä¢ In LMC, represented by two digits following the op code

‚Ä¢ Object to be manipulated

‚Ä¢  Data or

‚Ä¢  Address of data

Address            Content

Op code      Operand

Copyright 2010 John Wiley & Sons, Inc.                                        6-7


Magic!

‚Ä¢ Load program into memory

‚Ä¢ Put data into In Basket


Copyright 2010 John Wiley &
Sons, Inc.

6-8


Assembly Language

‚Ä¢ Specific to a CPU

‚Ä¢ 1 to 1 correspondence between assembly language instruction and
binary (machine) language instruction

‚Ä¢ Mnemonics (short character sequence) represent instructions

‚Ä¢ Used when programmer needs precise control over hardware, e.g.,
device drivers


Copyright 2010 John Wiley &
Sons, Inc.

6-9


Instruction Set

Arithmetic      1xx  ADD
2xx  SUB

Data Movement  3xx  STORE

5xx  LOAD

Input/Output    901  INPUT

902  Output


Machine Control
(coffee break)

000  HALT

COB

Copyright 2010 John Wiley & Sons, Inc.                                        6-10


Input/Output

‚Ä¢ Move data between calculator and in/out baskets


IN (input)

OUT (output)

Content

Op Code  Operand

(address)

9        01

9        02


Copyright 2010 John Wiley &
Sons, Inc.

6-11


LMC Input/Output

IN
OUT


Copyright 2010 John Wiley &
Sons, Inc.

6-12


Internal Data Movement

‚Ä¢ Between mailbox and calculator


STO

(store)

LDA (load)

Content

Op Code  Operand

(address)

3        xx

5        xx


Copyright 2010 John Wiley &
Sons, Inc.

6-13


LMC Internal Data

LDA
STO


Copyright 2010 John Wiley &
Sons, Inc.

6-14


Arithmetic Instructions

‚Ä¢ Read mailbox

‚Ä¢ Perform operation in the calculator


ADD
SUB

Content

Op Code      Operand
(address)

1           xx

2           xx

Copyright 2010 John Wiley & Sons, Inc.                                        6-15


LMC Arithmetic Instructions

ADD
SUB


Copyright 2010 John Wiley &
Sons, Inc.

6-16


Data storage location

‚Ä¢ Physically identical to instruction mailbox

‚Ä¢ Not located in instruction sequence

‚Ä¢ Identified by DAT mnemonic


Copyright 2010 John Wiley &
Sons, Inc.

6-17


Simple Program: Add 2 Numbers

‚Ä¢ Assume data is stored
in mailboxes with
addresses >90

‚Ä¢ Write instructions

Input a #

Store the #

Input a #

Add

Output the

number


Copyright 2010 John Wiley &
Sons, Inc.

6-18


Program to Add 2 Numbers


Mailbox
00

01

02

03

04

05

99

Code
901

399

901

199

902

000

000

Instruction Description

;input 1st Number

;store data

;input 2nd Number

;add 1st # to 2nd #

;output result

;stop

;data

Copyright 2010 John Wiley & Sons, Inc.                                        6-19


Program to Add 2 Numbers:

Using Mnemonics

Mailbox  Mnemonic Instruction Description
00    IN        ;input 1st Number

01    STO 99   ;store data

02    IN        ;input 2nd Number
03    ADD 99   ;add 1st # to 2nd #
04    OUT      ;output result

05    COB     ;stop

99    DAT 00   ;data

Copyright 2010 John Wiley & Sons, Inc.                                        6-20


Program Control

‚Ä¢ Branching (executing an instruction out of sequence)

‚Ä¢ Changes the address in the counter

‚Ä¢ Halt                               Content

Op Code       Operand

(address)

BR (Jump)                 6             xx

BRZ (Branch on 0)            7             xx

BRP (Branch on +)            8             xx

COB (stop)                 0           (ignore)


Copyright 2010 John Wiley &
Sons, Inc.

6-21


LMC Instruction Set

Arithmetic          1xx    ADD

2xx   SUB

Data Movement      3xx    STORE

5xx   LOAD

BR               6xx    JUMP

BRZ              7xx    BRANC ON 0

BRP              8xx    BRANCH ON +

Input/Output         901   INPUT

902   OUTPUT


Machine Control

(coffee break)

000   HALT

COB

Copyright 2010 John Wiley & Sons, Inc.                                        6-22


Find Positive Difference of 2 Numbers


00  IN

01  STO 10

02  IN

03  STO 11

04  SUB 10

05  BRP 08

06  LDA 10

07  SUB 11

08  OUT

09  COB

10  DAT 00

11  DAT 00

901

310

901

311

210

808

510

211

902

000

000

000

;test

;if negative, reverse order

;print result and

;stop

;used for data

;used for data

Copyright 2010 John Wiley & Sons, Inc.                                        6-23


Instruction Cycle

‚Ä¢ Fetch: Little Man finds out what instruction he is to execute

‚Ä¢ Execute: Little Man performs the work.


Copyright 2010 John Wiley &
Sons, Inc.

6-24


Fetch Portion of

Fetch and Execute Cycle

1. Little Man reads the
address from the
location counter

2. He walks over to
the mailbox that
corresponds to the
location counter


Copyright 2010 John Wiley &
Sons, Inc.

6-25


Fetch, cont.

3. And reads the
number on the slip
of paper (he puts
the slip back in case
he           needs to read it
again later)


Copyright 2010 John Wiley &
Sons, Inc.

6-26


Execute Portion

1. The Little Man goes to the
mailbox address specified

in the instruction he just
fetched.

2. He reads the number in that
mailbox (he remembers to
replace it in case he needs it
later).

Copyright 2010 John Wiley & Sons, Inc.                                        6-27


Execute, cont.

3. He walks over to the
calculator and punches the
number in.

4. He walks over to the location

counter and clicks it, which
gets him ready to fetch the
next instruction.

Copyright 2010 John Wiley & Sons, Inc.                                        6-28


von Neumann Architecture (1945)

‚Ä¢ Stored program concept

‚Ä¢ Memory is addressed linearly

‚Ä¢ Memory is addressed without regard to content


Copyright 2010 John Wiley &
Sons, Inc.

6-29


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work beyond that permitted
in section 117 of the 1976 United States Copyright Act without express permission
of the copyright owner is unlawful. Request for further information should be
addressed to the Permissions Department, John Wiley & Sons, Inc. The purchaser
may make back-up copies for his/her own use only and not for distribution or
resale. The Publisher assumes no responsibility for errors, omissions, or damages
caused by the use of these programs or from the use of the information contained
herein.‚Äù


Copyright 2010 John Wiley &
Sons, Inc.

6-30


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 7:

The CPU and Memory

The Architecture of Computer Hardware, Systems Software
& Networking:

An Information Technology Approach

4th Edition, Irv Englander
John                  Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong, Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-authored with Lynne Senne,
Bentley College


CPU: Major Components

‚Ä¢ ALU (arithmetic logic unit)

‚Ä¢ Performs calculations and comparisons

‚Ä¢ CU (control unit)

‚Ä¢ Performs fetch/execute cycle

‚Ä¢  Accesses program instructions and issues commands to the ALU

‚Ä¢  Moves data to and from CPU registers and other hardware components

‚Ä¢ Subcomponents:

‚Ä¢  Memory management unit: supervises fetching instructions and data from memory

‚Ä¢  I/O Interface: sometimes combined with memory management unit as Bus Interface

Unit


Copyright 2010 John Wiley &
Sons, Inc.

7-5


System Block Diagram


Copyright 2010 John Wiley &
Sons, Inc.

7-6


The Little Man Computer


Copyright 2010 John Wiley &
Sons, Inc.

7-7


Concept of Registers

‚Ä¢ Small, permanent storage locations within the CPU used for a
particular purpose

‚Ä¢ Manipulated directly by the Control Unit

‚Ä¢ Wired for specific function

‚Ä¢ Size in bits or bytes (not in MB like memory)

‚Ä¢ Can hold data, an address or an instruction

‚Ä¢ How many registers does the LMC have?

‚Ä¢ What are the registers in the LMC?


Copyright 2010 John Wiley &
Sons, Inc.

7-8


Registers

‚Ä¢ Use of Registers

‚Ä¢ Scratchpad for currently executing program

‚Ä¢  Holds data needed quickly or frequently

‚Ä¢ Stores information about status of CPU and currently executing program

‚Ä¢  Address of next program instruction

‚Ä¢  Signals from external devices

‚Ä¢ General Purpose Registers

‚Ä¢ User-visible registers

‚Ä¢ Hold intermediate results or data values, e.g., loop counters

‚Ä¢ Equivalent to LMC‚Äôs calculator

‚Ä¢ Typically several dozen in current CPUs


Copyright 2010 John Wiley &
Sons, Inc.

7-9


Special-Purpose Registers

‚Ä¢ Program Count Register (PC)

‚Ä¢ Also called instruction pointer

‚Ä¢ Instruction Register (IR)

‚Ä¢ Stores instruction fetched from memory

‚Ä¢ Memory Address Register (MAR)

‚Ä¢ Memory Data Register (MDR)

‚Ä¢ Status Registers

‚Ä¢ Status of CPU and currently executing program

‚Ä¢ Flags (one bit Boolean variable) to track condition like arithmetic carry and
overflow, power failure, internal computer error


Copyright 2010 John Wiley &
Sons, Inc.

7-10


Register Operations

‚Ä¢ Stores values from other locations (registers and memory)

‚Ä¢ Addition and subtraction

‚Ä¢ Shift or rotate data

‚Ä¢ Test contents for conditions such as zero or positive


Copyright 2010 John Wiley &
Sons, Inc.

7-11


Operation of Memory

‚Ä¢ Each memory location has a unique address

‚Ä¢ Address from an instruction is copied to the MAR which finds the
location in memory

‚Ä¢ CPU determines if it is a store or retrieval

‚Ä¢ Transfer takes place between the MDR and memory

‚Ä¢ MDR is a two way register


Copyright 2010 John Wiley &
Sons, Inc.

7-12


Relationship between MAR,
MDR and Memory

Address             Data


Copyright 2010 John Wiley &
Sons, Inc.

7-13


MAR-MDR Example


Copyright 2010 John Wiley &
Sons, Inc.

7-14


Visual Analogy of Memory


Copyright 2010 John Wiley &
Sons, Inc.

7-15


Individual Memory Cell


Copyright 2010 John Wiley &
Sons, Inc.

7-16


Memory Capacity

Determined by two factors

1. Number of bits in the MAR

‚Ä¢ LMC = 100 (00 to 99)

‚Ä¢ 2K where K = width of the register in bits

2. Size of the address portion of the instruction

‚Ä¢ 4 bits allows 16 locations

‚Ä¢ 8 bits allows 256 locations

‚Ä¢ 32 bits allows 4,294,967,296 or 4 GB


Copyright 2010 John Wiley &
Sons, Inc.

7-17


RAM: Random Access Memory

‚Ä¢ DRAM (Dynamic RAM)

‚Ä¢ Most common, cheap, less electrical power, less heat, smaller space

‚Ä¢ Volatile: must be refreshed (recharged with power) 1000‚Äôs of times each
second

‚Ä¢ SRAM (static RAM)

‚Ä¢ Faster and more expensive than DRAM

‚Ä¢ Volatile

‚Ä¢ Small amounts are often used in cache memory for high-speed memory
access


Copyright 2010 John Wiley &
Sons, Inc.

7-18


Nonvolatile Memory

‚Ä¢ ROM

‚Ä¢ Read-only Memory

‚Ä¢ Holds software that is not expected to change over the life of the system

‚Ä¢ EEPROM

‚Ä¢ Electrically Erasable Programmable ROM

‚Ä¢ Flash Memory

‚Ä¢ Faster than disks but more expensive

‚Ä¢ Uses hot carrier injection to store bits of data

‚Ä¢ Slow rewrite time compared to RAM

‚Ä¢ Useful for nonvolatile portable computer storage


Copyright 2010 John Wiley &
Sons, Inc.

7-19


Fetch-Execute Cycle

‚Ä¢ Two-cycle process because both instructions and data are in memory

‚Ä¢ Fetch

‚Ä¢ Decode or find instruction, load from memory into register and
signal ALU

‚Ä¢ Execute

‚Ä¢ Performs operation that instruction requires

‚Ä¢ Move/transform data


Copyright 2010 John Wiley &
Sons, Inc.

7-20


LMC vs. CPU

Fetch and Execute Cycle


Copyright 2010 John Wiley &
Sons, Inc.

7-21


Load Fetch/Execute Cycle

1. PC ü°™ MAR        Transfer the address from the

PC to the MAR

2. MDR ü°™ IR         Transfer the instruction to the
IR

3. IR[address] ü°™ MAR  Address portion of the

instruction loaded in MAR

4. MDR ü°™ A         Actual data copied into the
accumulator

5. PC + 1 ü°™ PC       Program Counter incremented

Copyright 2010 John Wiley & Sons, Inc.                                        7-22


Store Fetch/Execute Cycle

1. PC ü°™ MAR        Transfer the address from the

PC to the MAR

2. MDR ü°™ IR         Transfer the instruction to the
IR

3. IR[address] ü°™ MAR  Address portion of the

instruction loaded in MAR

4.  A ü°™ MDR*        Accumulator copies data into

MDR

5.  PC + 1 ü°™ PC      Program Counter incremented

*Notice how Step #4 differs for LOAD and STORE

Copyright 2010 John Wiley & Sons, Inc.                                        7-23


ADD Fetch/Execute Cycle

1. PC ü°™ MAR        Transfer the address from the

PC to the MAR

2. MDR ü°™ IR         Transfer the instruction to the
IR

3. IR[address] ü°™ MAR  Address portion of the

instruction loaded in MAR

4. A + MDR ü°™ A      Contents of MDR added to

contents of accumulator

5. PC + 1 ü°™ PC       Program Counter incremented

Copyright 2010 John Wiley & Sons, Inc.                                        7-24


LMC Fetch/Execute


SUBTRACT
PC ü°™ MAR
MDR ü°™ IR

IR[addr] ü°™ MAR

A ‚Äì MDR ü°™ A

PC + 1 ü°™ PC

IN

PC ü°™ MAR
MDR ü°™ IR
IOR ü°™ A

PC + 1 ü°™ PC

OUT

PC ü°™ MAR
MDR ü°™ IR
A ü°™ IOR

PC + 1 ü°™ PC

HALT

PC ü°™ MAR
MDR ü°™ IR


BRANCH
PC ü°™ MAR
MDR ü°™ IR

IR[addr] ü°™ PC

BRANCH on Condition

PC ü°™ MAR
MDR ü°™ IR

If condition false: PC + 1 ü°™ PC
If condition true: IR[addr] ü°™ PC


Copyright 2010 John Wiley &
Sons, Inc.

7-25


Bus

‚Ä¢ The physical connection that makes it possible to transfer data from one
location in the computer system to another

‚Ä¢ Group of electrical or optical conductors for carrying signals from one
location to another

‚Ä¢ Wires or conductors printed on a circuit board

‚Ä¢ Line: each conductor in the bus

‚Ä¢ 4 kinds of signals

1. Data

2. Addressing

3. Control signals

4. Power (sometimes)


Copyright 2010 John Wiley &
Sons, Inc.

7-26


Bus Characteristics

‚Ä¢ Number of separate conductors

‚Ä¢ Data width in bits carried simultaneously

‚Ä¢ Addressing capacity

‚Ä¢ Lines on the bus are for a single type of signal or shared

‚Ä¢ Throughput - data transfer rate in bits per second

‚Ä¢ Distance between two endpoints

‚Ä¢ Number and type of attachments supported

‚Ä¢ Type of control required

‚Ä¢ Defined purpose

‚Ä¢ Features and capabilities


Copyright 2010 John Wiley &
Sons, Inc.

7-27


Bus Categorizations

‚Ä¢ Parallel vs. serial buses

‚Ä¢ Direction of transmission

‚Ä¢ Simplex ‚Äì unidirectional

‚Ä¢ Half duplex ‚Äì bidirectional, one direction at a time

‚Ä¢ Full duplex ‚Äì bidirectional simultaneously

‚Ä¢ Method of interconnection

‚Ä¢ Point-to-point ‚Äì single source to single destination

‚Ä¢  Cables ‚Äì point-to-point buses that connect to an external device

‚Ä¢ Multipoint bus ‚Äì also broadcast bus or multidrop bus

‚Ä¢  Connect multiple points to one another


Copyright 2010 John Wiley &
Sons, Inc.

7-28


Parallel vs. Serial Buses

‚Ä¢ Parallel

‚Ä¢ High throughput because all bits of a word are transmitted

simultaneously

‚Ä¢ Expensive and require a lot of space

‚Ä¢ Subject to radio-generated electrical interference which limits

their speed and length

‚Ä¢ Generally used for short distances such as CPU buses and on
computer motherboards

‚Ä¢ Serial

‚Ä¢ 1 bit transmitted at a timed

‚Ä¢ Single data line pair and a few control lines

‚Ä¢  For many applications, throughput is higher than for parallel
because of the lack of electrical interference


Copyright 2010 John Wiley &
Sons, Inc.

7-29


Point-to-point vs. Multipoint


Plug-in

device

Broadcast
bus
Example:
Ethernet


Copyright 2010 John Wiley &
Sons, Inc.

Shared among

multiple devices

7-30


Classification of Instructions

‚Ä¢ Data Movement (load, store)

‚Ä¢ Most common, greatest flexibility

‚Ä¢ Involve memory and registers

‚Ä¢ What‚Äôs this size of a word ? 16? 32? 64 bits?

‚Ä¢ Arithmetic

‚Ä¢ Operators + - / * ^

‚Ä¢ Integers and floating point

‚Ä¢ Boolean Logic

‚Ä¢ Often includes at least AND, XOR, and NOT

‚Ä¢ Single operand manipulation instructions

‚Ä¢ Negating, decrementing, incrementing, set to 0


Copyright 2010 John Wiley &
Sons, Inc.

7-31


More Instruction Classifications

‚Ä¢ Bit manipulation instructions

‚Ä¢ Flags to test for conditions

‚Ä¢ Shift and rotate

‚Ä¢ Program control

‚Ä¢ Stack instructions

‚Ä¢ Multiple data instructions

‚Ä¢ I/O and machine control


Copyright 2010 John Wiley &
Sons, Inc.

7-32


Register Shifts and Rotates


Copyright 2010 John Wiley &
Sons, Inc.

7-33


Program Control Instructions

‚Ä¢ Program control

‚Ä¢ Jump and branch

‚Ä¢ Subroutine call
and return


Copyright 2010 John Wiley &
Sons, Inc.

7-34


Stack Instructions

‚Ä¢ Stack instructions

‚Ä¢ LIFO method for organizing information

‚Ä¢ Items removed in the reverse order from that in which they are
added

Push                Pop

Copyright 2010 John Wiley & Sons, Inc.                                        7-35


Fixed Location Subroutine
Return Address Storage: Oops!


Copyright 2010 John Wiley &
Sons, Inc.

7-36


Stack Subroutine Return Address Storage


Copyright 2010 John Wiley &
Sons, Inc.

7-37


Block of Memory as a Stack


Copyright 2010 John Wiley &
Sons, Inc.

7-38


Multiple Data Instructions

‚Ä¢ Perform a single operation on multiple pieces of data
simultaneously

‚Ä¢ SIMD: Single Instruction, Multiple Data

‚Ä¢ Commonly used in multimedia, vector and array processing
applications


Copyright 2010 John Wiley &
Sons, Inc.

7-39



Instruction Elements

‚Ä¢ OPCODE: task

‚Ä¢ Source OPERAND(s)

‚Ä¢ Result OPERAND

Addresses

‚Ä¢ Location of data (register, memory)

‚Ä¢ Explicit: included in instruction

‚Ä¢ Implicit: default assumed


OPCODE

Source
OPERAND

Result
OPERAND


Copyright 2010 John Wiley &
Sons, Inc.

7-40


Instruction Format

‚Ä¢ Machine-specific template that specifies

‚Ä¢ Length of the op code

‚Ä¢ Number of operands

‚Ä¢ Length of operands

Simple
32-bit

Instruction

Format

Copyright 2010 John Wiley & Sons, Inc.                                        7-41


Instructions

‚Ä¢ Instruction

‚Ä¢ Direction given to a computer

‚Ä¢ Causes electrical or optical signals to be sent through specific circuits for processing

‚Ä¢ Instruction set

‚Ä¢ Design defines functions performed by the processor

‚Ä¢ Differentiates computer architecture by the

‚Ä¢  Number of instructions

‚Ä¢  Complexity of operations performed by individual instructions

‚Ä¢  Data types supported

‚Ä¢  Format (layout, fixed vs. variable length)

‚Ä¢  Use of registers

‚Ä¢  Addressing (size, modes)


Copyright 2010 John Wiley &
Sons, Inc.

7-42


Instruction Word Size

‚Ä¢ Fixed vs. variable size

‚Ä¢ Pipelining has mostly eliminated variable instruction size architectures

‚Ä¢ Most current architectures use 32-bit or 64-bit words

‚Ä¢ Addressing Modes

‚Ä¢ Direct

‚Ä¢  Mode used by the LMC

‚Ä¢ Register Deferred

‚Ä¢ Also immediate, indirect, indexed


Copyright 2010 John Wiley &
Sons, Inc.

7-43


Instruction Format Examples


Copyright 2010 John Wiley &
Sons, Inc.

7-44


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work beyond that permitted
in section 117 of the 1976 United States Copyright Act without express permission
of the copyright owner is unlawful. Request for further information should be
addressed to the Permissions Department, John Wiley & Sons, Inc. The purchaser
may make back-up copies for his/her own use only and not for distribution or
resale. The Publisher assumes no responsibility for errors, omissions, or damages
caused by the use of these programs or from the use of the information contained
herein.‚Äù


Copyright 2010 John Wiley &
Sons, Inc.

7-45


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 8: CPU and Memory

Design, Enhancement, and Implementation

The Architecture of Computer Hardware, Systems Software
& Networking:

An Information Technology Approach

4th Edition, Irv Englander
John                  Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong, Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-authored with Lynne Senne,
Bentley College


Current CPU Architectures

‚Ä¢ Current CPU Architecture Designs

‚Ä¢ Traditional modern architectures

‚Ä¢ VLIW (Transmeta) ‚Äì Very Long Instruction Word

‚Ä¢ EPIC (Intel) ‚Äì Explicitly Parallel Instruction Computer

‚Ä¢ Current CPU Architectures

‚Ä¢ IBM Mainframe series

‚Ä¢ Intel x86 family

‚Ä¢ IBM POWER/PowerPC family

‚Ä¢ Sun SPARC family


Copyright 2010 John Wiley &
Sons, Inc.

8-5


Traditional Modern Architectures

Problems with early CPU Architectures and solutions:

‚Ä¢ Large number of specialized instructions were rarely used but added hardware

complexity and slowed down other instructions

‚Ä¢ Slow data memory accesses could be reduced by increasing the number of
general purpose registers

‚Ä¢ Using general registers to hold addresses could reduce the number of addressing

modes and simplify architecture design

‚Ä¢ Fixed-length, fixed format instruction words would allow instructions to be
fetched and decoded independently and in parallel


Copyright 2010 John Wiley &
Sons, Inc.

8-6


VLIW Architecture

‚Ä¢ Transmeta Crusoe CPU

‚Ä¢ 128-bit instruction bundle = molecule

‚Ä¢ Four 32-bit atoms (atom = instruction)

‚Ä¢ Parallel processing of 4 instructions

‚Ä¢ 64 general purpose registers

‚Ä¢ Code morphing layer

‚Ä¢ Translates instructions written for other CPUs into molecules

‚Ä¢ Instructions are not written directly for the Crusoe CPU


Copyright 2010 John Wiley &
Sons, Inc.

8-7


EPIC Architecture

‚Ä¢ 128-bit instruction bundle

‚Ä¢ 3 41-bit instructions

‚Ä¢ 5 bits to identify type of instructions in bundle

‚Ä¢ 128 64-bit general purpose registers

‚Ä¢ 128 82-bit floating point registers

‚Ä¢ Intel X86 instruction set included

‚Ä¢ Programmers and compilers follow guidelines to ensure parallel
execution of instructions


Copyright 2010 John Wiley &
Sons, Inc.

8-8


Fetch-Execute Cycle Timing Issues

‚Ä¢ Computer clock is used for timing purposes for each step of
the instruction cycle

‚Ä¢ GHz (gighertz) ‚Äì billion steps per second

‚Ä¢ Instructions can (and often) take more than one step

‚Ä¢ Data word width can require multiple steps

Fetch-execute
timing diagram


Copyright 2010 John Wiley &
Sons, Inc.

8-9


CPU Features and Enhancements

‚Ä¢ Separate Fetch/Execute Units

‚Ä¢ Pipelining

‚Ä¢ Multiple, Parallel Execution Units

‚Ä¢ Scalar Processing

‚Ä¢ Superscalar Processing

‚Ä¢ Branch Instruction Processing


Copyright 2010 John Wiley &
Sons, Inc.

8-10


Separate Fetch-Execute Units

‚Ä¢ Fetch Unit

‚Ä¢ Instruction fetch unit

‚Ä¢ Instruction decode unit

‚Ä¢  Determine opcode

‚Ä¢  Identify type of instruction and operands

‚Ä¢ Several instructions are fetched in parallel and held in a buffer until decoded and

executed

‚Ä¢ IP ‚Äì Instruction Pointer register holds instruction location of current being processed

‚Ä¢ Execute Unit

‚Ä¢ Receives instructions from the decode unit

‚Ä¢ Appropriate execution unit services the instruction


Copyright 2010 John Wiley &
Sons, Inc.

8-11


Alternative CPU Organization


Copyright 2010 John Wiley &
Sons, Inc.

8-12


Instruction Pipelining

‚Ä¢ Assembly-line technique to allow overlapping
between fetch-execute cycles of sequences of
instructions

‚Ä¢ Scalar processing

‚Ä¢ Average instruction execution is approximately equal to

the clock speed of the CPU

‚Ä¢ Problems from stalling

‚Ä¢ Instructions have different numbers of steps

‚Ä¢ Problems from branching


Copyright 2010 John Wiley &
Sons, Inc.

8-13


Pipelining Example


Copyright 2010 John Wiley &
Sons, Inc.

8-14


Branch Problem Solutions

‚Ä¢ Separate pipelines for both possibilities

‚Ä¢ Probabilistic approach

‚Ä¢ Requiring the following instruction to not be dependent on the branch

‚Ä¢ Instruction Reordering (superscalar processing)


Copyright 2010 John Wiley &
Sons, Inc.

8-15


Multiple, Parallel Execution Units

‚Ä¢ Different instructions have different numbers of steps in their cycle

‚Ä¢ Differences in each step

‚Ä¢ Each execution unit is optimized for one general type of instruction

‚Ä¢ Multiple execution units permit simultaneous execution of several
instructions


Copyright 2010 John Wiley &
Sons, Inc.

8-16


Superscalar Processing

‚Ä¢ Process more than one instruction per clock cycle

‚Ä¢ Separate fetch and execute cycles as much as possible

‚Ä¢ Buffers for fetch and decode phases

‚Ä¢ Parallel execution units


Copyright 2010 John Wiley &
Sons, Inc.

8-17


Superscalar CPU Block Diagram


Copyright 2010 John Wiley &
Sons, Inc.

8-18


Scalar vs. Superscalar Processing


Copyright 2010 John Wiley &
Sons, Inc.

8-19


Superscalar Issues

‚Ä¢ Out-of-order processing ‚Äì dependencies (hazards)

‚Ä¢ Data dependencies

‚Ä¢ Branch (flow) dependencies and speculative execution

‚Ä¢ Parallel speculative execution or branch prediction

‚Ä¢ Branch History Table

‚Ä¢ Register access conflicts

‚Ä¢ Rename or logical registers


Copyright 2010 John Wiley &
Sons, Inc.

8-20


Memory Enhancements

‚Ä¢ Memory is slow compared to CPU processing speeds!

‚Ä¢ 2Ghz CPU = 1 cycle in ¬Ω of a billionth of a second

‚Ä¢ 70ns DRAM = 1 access in 70 millionth of a second

‚Ä¢ Methods to improvement memory accesses

‚Ä¢ Wide Path Memory Access

‚Ä¢  Retrieve multiple bytes instead of 1 byte at a time

‚Ä¢ Memory Interleaving

‚Ä¢  Partition memory into subsections, each with its own address register and data register

‚Ä¢ Cache Memory


Copyright 2010 John Wiley &
Sons, Inc.

8-21


Memory Interleaving


Copyright 2010 John Wiley &
Sons, Inc.

8-22


Cache Memory

‚Ä¢ Blocks: 8 or 16 bytes

‚Ä¢ Tags: pointer to location in main memory

‚Ä¢ Cache controller

‚Ä¢ hardware that checks tags

‚Ä¢ Cache Line

‚Ä¢ Unit of transfer between storage and cache memory

‚Ä¢ Hit Ratio: ratio of hits out of total requests

‚Ä¢ Synchronizing cache and memory

‚Ä¢ Write through

‚Ä¢ Write back


Copyright 2010 John Wiley &
Sons, Inc.

8-23


Step-by-Step Use of Cache


Copyright 2010 John Wiley &
Sons, Inc.

8-24


Step-by-Step Use of Cache


Copyright 2010 John Wiley &
Sons, Inc.

8-25


Performance Advantages

‚Ä¢ Hit ratios of 90% common

‚Ä¢ 50%+ improved execution speed

‚Ä¢ Locality of reference is why caching works

‚Ä¢ Most memory references confined to small region of

memory at any given time

‚Ä¢ Well-written program in small loop, procedure or
function

‚Ä¢ Data likely in array

‚Ä¢ Variables stored together


Copyright 2010 John Wiley &
Sons, Inc.

8-26


Two-level Caches

‚Ä¢ Why do the sizes of the caches have to be different?


Copyright 2010 John Wiley &
Sons, Inc.

8-27


Modern CPU Block Diagram


Copyright 2010 John Wiley &
Sons, Inc.

8-28


Multiprocessing

‚Ä¢ Reasons

‚Ä¢ Increase the processing power of a system

‚Ä¢ Parallel processing

‚Ä¢ Multiprocessor system

‚Ä¢ Tightly coupled

‚Ä¢ Multicore processors - when CPUs are on a single integrated circuit


Copyright 2010 John Wiley &
Sons, Inc.

8-29


Multiprocessor Systems

‚Ä¢ Identical access to programs, data, shared memory, I/O, etc.

‚Ä¢ Easily extends multi-tasking, and redundant program execution

‚Ä¢ Two ways to configure

‚Ä¢ Master-slave multiprocessing

‚Ä¢ Symmetrical multiprocessing (SMP)


Copyright 2010 John Wiley &
Sons, Inc.

8-30


Typical Multiprocessing System Configuration


Copyright 2010 John Wiley &
Sons, Inc.

8-31


Master-Slave Multiprocessing


‚Ä¢ Master CPU

‚Ä¢ Manages the system

‚Ä¢ Controls all resources and scheduling

‚Ä¢ Assigns tasks to slave CPUs

‚Ä¢ Advantages

‚Ä¢ Simplicity

‚Ä¢ Protection of system and data

‚Ä¢ Disadvantages

‚Ä¢ Master CPU becomes a bottleneck

‚Ä¢ Reliability issues ‚Äì if master CPU fails entire system fails

Copyright 2010 John Wiley &
Sons, Inc.

8-32


Symmetrical Multiprocessing

‚Ä¢ Each CPU has equal access to resources

‚Ä¢ Each CPU determines what to run using a standard algorithm

‚Ä¢ Disadvantages

‚Ä¢ Resource conflicts ‚Äì memory, i/o, etc.

‚Ä¢ Complex implementation

‚Ä¢ Advantages

‚Ä¢ High reliability

‚Ä¢ Fault tolerant support is straightforward

‚Ä¢ Balanced workload


Copyright 2010 John Wiley &
Sons, Inc.

8-33


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work beyond that permitted
in section 117 of the 1976 United States Copyright Act without express permission
of the copyright owner is unlawful. Request for further information should be
addressed to the Permissions Department, John Wiley & Sons, Inc. The purchaser
may make back-up copies for his/her own use only and not for distribution or
resale. The Publisher assumes no responsibility for errors, omissions, or damages
caused by the use of these programs or from the use of the information contained
herein.‚Äù


Copyright 2010 John Wiley &
Sons, Inc.

8-34


Thank You



The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


College of Computing and Informatics
Computer Organization


CHAPTER 9: Input / Output


Basic Model

Processing speed or program execution

determined primarily by ability of I/O operations to stay ahead of processor.

Input    Process    Output


I/O Requirements

‚Ä¢ Means for addressing different peripheral devices

‚Ä¢ A way for peripheral devices to initiate communication with the CPU

‚Ä¢ An efficient means of transferring data directly between I/O and
memory for large data transfers since programmed I/O is suitable only
for slow devices and individual word transfers

‚Ä¢ Buses that interconnect high-speed I/O devices with the computer
must support high data transfer rates

‚Ä¢ Means for handling devices with extremely different control

requirements


I/O Interfaces

‚Ä¢ Are necessary because of

‚Ä¢ Different formats required by the devices

‚Ä¢ Incompatibilities in speed between the devices and the CPU make
synchronization difficult

‚Ä¢ Bursts of data vs. streaming data

‚Ä¢ Device control requirements that would tie up too much CPU time


Examples of I/O Devices


Simple I/O Configuration


More Complex I/O Module


Advanced I/O Techniques

‚Ä¢ Programmed I/O

‚Ä¢ CPU controlled I/O

‚Ä¢ Interrupt Driven I/O

‚Ä¢ External input controls

‚Ä¢ Direct Memory Access Controllers

‚Ä¢ Method for transferring data between main memory and
a device that bypasses the CPU


Programmed I/O

‚Ä¢ I/O data and address registers in CPU

‚Ä¢ One word transfer per I/O instruction

‚Ä¢ Address information for each I/O device

‚Ä¢ LMC I/O capability for 100 devices

‚Ä¢ Full instruction fetch/execute cycle

‚Ä¢ Primary use:

‚Ä¢ keyboards

‚Ä¢ communication with I/O modules (see DMA)


Programmed I/O Example


Programmed I/O Example


Interrupts

‚Ä¢ Signal that causes the CPU to alter its normal flow of instruction

execution

‚Ä¢ frees CPU from waiting for events

‚Ä¢ provides control for external I/O initiation

‚Ä¢ Examples

‚Ä¢ unexpected input

‚Ä¢ abnormal situation

‚Ä¢ illegal instructions

‚Ä¢ multitasking, multiprocessing


Interrupt Terminology

‚Ä¢ Interrupt lines (hardware)

‚Ä¢ One or more special control lines to the CPU

‚Ä¢ Interrupt request

‚Ä¢ Interrupt handlers

‚Ä¢ Program that services the interrupt

‚Ä¢ Also known as an interrupt routine or device driver

‚Ä¢ Context

‚Ä¢ Saved registers of a program before control is transferred to the
interrupt handler

‚Ä¢ Allows program to resume exactly where it left off when control
returns to interrupted program


Use of Interrupts

‚Ä¢ Notify that an external event has occurred

‚Ä¢ real-time or time-sensitive

‚Ä¢ Signal completion

‚Ä¢ printer ready or buffer full

‚Ä¢ Allocate CPU time

‚Ä¢ time sharing

‚Ä¢ Indicate abnormal event (CPU originates for
notification and recovery)

‚Ä¢ illegal operation, hardware error

‚Ä¢ Software interrupts



The CPU - The Interrupt Cycle

‚Ä¢ Fetch / Execute cycle

‚Ä¢ Interrupt cycle

HALT

Interrupts Disabled

Process

Interrupt

START

Fetch Next
Instruction

Execute

Instruction

Check for

Interrupt


Servicing the Interrupt

1.  Lower priority interrupts are held until higher priority interrupts
are complete

2.  Suspend program in progress

3.  Save context, including last instruction executed and data values
in registers, in the PCB or the stack area in memory

4.  Branch to interrupt handler program


Servicing an Interrupt


Interrupt Processing Methods

‚Ä¢ Vectored interrupt

‚Ä¢ Address of interrupting device is included in the interrupt

‚Ä¢ Requires additional hardware to implement

‚Ä¢ Polling

‚Ä¢ Identifies interrupting device by polling each device

‚Ä¢ General interrupt is shared by all devices


Vectored Interrupts


Polled Interrupts


Print Handler Interrupt


Using an Interrupt for Time Sharing


Multiple Interrupts Example


Direct Memory Access

‚Ä¢ Transferring large blocks of data

‚Ä¢ Direct transfer to and from memory

‚Ä¢ CPU not actively involved in transfer itself

‚Ä¢ Required conditions for DMA

‚Ä¢ The I/O interface and memory must be connected

‚Ä¢ The I/O module must be capable of reading and writing to memory

‚Ä¢ Conflicts between the CPU and the I/O module must be avoided

‚Ä¢ Interrupt required for completion


DMA Instructions

‚Ä¢   Application program requests I/O service from operating system

‚Ä¢  privileged programmed I/O instructions

‚Ä¢   To initiate DMA, programmed I/O is used to send the following
information:

1.  location of data on I/O device

2.  the starting location in memory

3.  the size of the block

4.  read/write

‚Ä¢   Interrupt to CPU upon completion of DMA


DMA Initiation and Control


I/O Module Interfaces


I/O Module Functions

‚Ä¢ Recognizes messages from device(s) addressed to it and accepts commands
from the CPU

‚Ä¢ Provides a buffer where the data from memory can be held until it can be
transferred to the device

‚Ä¢ Provides the necessary registers and controls to perform a direct memory

transfer

‚Ä¢ Physically controls the device

‚Ä¢ Copies data from its buffer to the device/from the CPU to its buffer

‚Ä¢ Communicates with CPU


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work
beyond that permitted in section 117 of the 1976 United
States Copyright Act without express permission of the
copyright owner is unlawful. Request for further information
should be addressed to the Permissions Department, John
Wiley & Sons, Inc. The purchaser may make back-up copies
for his/her own use only and not for distribution or resale.

The Publisher assumes no responsibility for errors,

omissions, or damages caused by the use of these programs
or from the use of the information contained herein.‚Äù


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


CHAPTER 10: Computer Peripherals


Peripherals

‚Ä¢ Devices that are separate from the basic computer

‚Ä¢ Not the CPU, memory, or power supply

‚Ä¢ Classified as input, output, and storage

‚Ä¢ Connect via

‚Ä¢ Ports

‚Ä¢ Interface to systems bus


Storage Devices

‚Ä¢ Primary memory

‚Ä¢ Secondary storage

‚Ä¢ Data and programs must be copied to primary memory for
CPU access

‚Ä¢ Permanence of data - nonvolatile

‚Ä¢ Direct access storage devices (DASDs)

‚Ä¢ Online storage

‚Ä¢ Offline storage ‚Äì loaded when needed

‚Ä¢ Network file storage

‚Ä¢ File servers, web servers, database servers


Speed

‚Ä¢ Measured by access time and data transfer rate

‚Ä¢ Access time: average time it takes a computer to locate data and
read it

‚Ä¢ millisecond = one-thousandth of a second

‚Ä¢ Data transfer rate: amount of data that moves per second


Storage Hierarchy


Secondary Storage Devices

‚Ä¢ Solid state memory

‚Ä¢ Magnetic disks

‚Ä¢ Optical disk storage

‚Ä¢ Magnetic tape

‚Ä¢ Network storage

‚Ä¢ Characteristics

‚Ä¢ Rotation vs. Linear

‚Ä¢ Direct access vs. Sequential access


Flash Memory

‚Ä¢ Nonvolatile electronic integrated circuit memory

‚Ä¢ Similar to other read-only memory but uses a different technology

‚Ä¢ Permits reading and writing individual bytes or small blocks of data

‚Ä¢ Small size makes it useful in portable devices such as USB ‚Äúthumb drives‚Äù,

digital cameras, cell phones, music players

‚Ä¢ Relatively immune to physical shocks

‚Ä¢ Generates little heat or noise


Disk Layouts ‚Äì CAV vs. CLV

‚Ä¢ CAV ‚Äì Constant Angular Velocity

‚Ä¢ Number of bits on each track is the same! Denser towards the center.

‚Ä¢ Spins the same speed for every track

‚Ä¢ CLV ‚Äì Constant Linear Velocity

‚Ä¢ All tracks have the same physical length and number of bits

‚Ä¢ Constant speed reading data off a track

‚Ä¢ Drive has to speed up when accessing close to the center of the drive
and slow down when accessing towards the edge of the drive


Disk Layout ‚Äì Multiple Zone

‚Ä¢ Multiple zone recording

‚Ä¢ Also known as zone bit recording (ZBR) or zone-CAV recording (Z-CAV)

‚Ä¢ Compromise between CAV and CLV

‚Ä¢ Disk divided into zones

‚Ä¢ Cylinders in different zones have a different number of sectors

‚Ä¢ Number of sectors in a particular zone is constant

‚Ä¢ Data is buffered so the data rate to the I/O interface is constant


Multiple-Zone Disk Configuration


Magnetic Disks

‚Ä¢ Track ‚Äì circle

‚Ä¢ Cylinder ‚Äì same track on all platters

‚Ä¢ Block ‚Äì small arc of a track

‚Ä¢ Sector ‚Äì pie-shaped part of a platter

‚Ä¢ Head ‚Äì reads data off the disk as disk rotates at high speed (4200-14000 RPM)

‚Ä¢ Head crash

‚Ä¢ Disk damaged if head touches disk surface

‚Ä¢ Parked heads


A Hard Disk Layout


Locating a Block of Data

‚Ä¢ Average seek time: time required to move

from one track to another

‚Ä¢ Latency: time required for disk to rotate to
beginning of correct sector

‚Ä¢ Transfer time: time required to transfer a

block of data to the disk controller buffer


Disk Access Times

‚Ä¢ Average Seek time

‚Ä¢ average time to move from one track to another

‚Ä¢ Average Latency time

‚Ä¢ average time to rotate to the beginning of the sector

‚Ä¢ Average Latency time = ¬Ω * 1/rotational speed

‚Ä¢ Transfer time

‚Ä¢ 1/(# of sectors * rotational speed)

‚Ä¢ Total Time to access a disk block

‚Ä¢ Avg. seek time + avg. latency time + avg. transfer time


Magnetic Disks

‚Ä¢ Data Block Format

‚Ä¢ Interblock gap

‚Ä¢ Header

‚Ä¢ Data

‚Ä¢ Formatting disk

‚Ä¢ Establishes the track positions, blocks and headers needed
before use of the disk


Disk Block Formats

Single Data Block

Header for Windows disk


Disk Arrays

‚Ä¢ Grouping of multiple disks together

‚Ä¢ RAID ‚Äì Redundant Array of Inexpensive Disks

‚Ä¢ Mirrored array

‚Ä¢ Striped array

‚Ä¢ RAID 0 to RAID 5


RAID ‚Äì Mirrored

‚ñ™ Pair of disks contain the exact same stores of data

‚ñ™ Reading data ‚Äì alternate blocks of data are read from hard drives
and combined

‚ñ™ Access time is reduced by approximately a factor equal to the
number of disk drives in array

‚ñ™ Read failure ‚Äì block is marked and then read from the mirrored
drive

‚ñ™ When using three or more mirrored drives, majority logic is used
in the event of a failure. Fault-tolerant computers use this
technique.


RAID - Striped

‚Ä¢ A file segment is stored divided into blocks on different disks

‚Ä¢ Minimum of three drives needed because one disk drive is reserved
for error checking

‚Ä¢ Writes ‚Äì block of parity words from each block of data is created
and put on the reserved error checking disk

‚Ä¢ Reads ‚Äì parity data is used to check original data


RAID Levels

‚Ä¢ RAID 0 ‚Äì not true RAID, no error checking or redundancy, but data
is placed across all drives for increased speed

‚Ä¢ RAID 1 ‚Äì mirrored array

‚Ä¢ RAID 2, 3, 4 ‚Äì arrays that are striped in different ways

‚Ä¢ RAID 5 ‚Äì error checking blocks are spread across all drives


Optical Storage

‚Ä¢ Reflected light off a mirrored or pitted surface

‚Ä¢ CD-ROM

‚Ä¢ 650 MB of data, approximately 550 MB after formatting and error checking

‚Ä¢ Spiral 3 miles long, containing 15 billion bits!

‚Ä¢ CLV ‚Äì all blocks are same physical length

‚Ä¢ Block ‚Äì 2352 bytes

‚Ä¢  2k of data (2048 bytes)

‚Ä¢  16 bytes for header (12 start, 4 id)

‚Ä¢  288 bytes for advanced error control

‚Ä¢ DVD ‚Äì similar technology to CD-ROM

‚Ä¢ WORM ‚Äì write-once read-many


Optical Storage

‚Ä¢ Laser strikes land: light reflected into detector

‚Ä¢ Laser strikes a pit: light scattered


Layout: CD-ROM vs. Standard Disk

CD-ROM                            Hard Disk


Types of Optical Storage

‚Ä¢ WORM Disks

‚Ä¢ Write-once-read-many times

‚Ä¢ Medium can be altered by using a medium-powered laser to blister the surface

‚Ä¢ Medium-powered laser blister technology also used for

‚Ä¢ CD-R, DVD-R, DVD-R, DVD+R

‚Ä¢ CD-RW, DVD-RW, DVD+RW, DVD-RAM, DVD+RAMBD-RE

‚Ä¢ File compatibility issues between the different CD, DVD and WORM formats


Magnetic Tape

‚Ä¢ Offline storage

‚Ä¢ Archival purposes

‚Ä¢ Disaster recovery

‚Ä¢ Tape Cartridges

‚Ä¢ Linear tape open format vs. helical scan tape format


Displays

‚Ä¢ Pixel ‚Äì picture element

‚Ä¢ Screen Size: diagonal length of screen

‚Ä¢ Aspect ratio ‚Äì X pixels to Y pixels

‚Ä¢ 4:3 ‚Äì older displays

‚Ä¢ 16:9 ‚Äì widescreen displays

‚Ä¢ Pixel color is determined by intensity of 3

colors ‚Äì Red, Green and Blue (RGB)

‚Ä¢ True Color ‚Äì 8 bits for each color

‚Ä¢ 256 levels of intensity for each color

‚Ä¢ 256 * 256 * 256 = 16.7 million colors


Resolution and Picture Size

‚Ä¢ Resolution

‚Ä¢ Measured as either number of pixels per inch or size of an individual pixel

‚Ä¢ Screen resolution examples:

‚Ä¢  768 x 1024

‚Ä¢  1440 x 900

‚Ä¢  1920 x 1080

‚Ä¢ Picture size calculation

‚Ä¢ Resolution * bits required to represent number of colors in picture

‚Ä¢  Example: resolution is 100 pixels by 50 pixels, 4 bits required for a 16 color image
100 * 50 * 4 bits = 20,000 bits

‚Ä¢ Video memory requirements are significant!


Interlaced vs. Progressive Scan


Diagram of Raster Screen Generation Process


Color Transformation Table


Display Example


LCD ‚Äì Liquid Crystal Display

‚Ä¢ Fluorescent light or LED panel

‚Ä¢ 3 color cells per pixel

‚Ä¢ Operation

‚Ä¢ 1À¢·µó filter polarizes light in a specific direction

‚Ä¢ Electric charge rotates molecules in liquid crystal cells
proportional to the strength of colors

‚Ä¢ Color filters only let through red, green, and blue light

‚Ä¢ Final filter lets through the brightness of light proportional
to                                            the polarization twist


Liquid Crystal Display


LCDs (continued)

‚Ä¢ Active matrix

‚Ä¢ One transistor per cell

‚Ä¢ More expensive

‚Ä¢ Brighter picture

‚Ä¢ Passive matrix

‚Ä¢ One transistor per row or column

‚Ä¢ Each cell is lit in succession

‚Ä¢ Display is dimmer since pixels are lit less
frequently


CRT Display Technology

‚Ä¢ CRTs (similar to TVs)

‚Ä¢ 3 stripes of phosphors for each color

‚Ä¢ 3 separate electron guns for each color

‚Ä¢ Strength of beam ü°™ brightness of color

‚Ä¢ Raster scan

‚Ä¢  30x per second

‚Ä¢  Interlaced vs. non-interlaced (progressive scan)


OLED Display Technology

‚Ä¢ No backlight

‚Ä¢ Consists of red, green and blue LEDs

‚Ä¢ Each LED lights up individually

‚Ä¢ Very thin displays with panels less than 3mm thick!


Printers

‚Ä¢ Dots vs. pixels

‚Ä¢ 300-2400 dpi vs. 70-100 pixels per inch

‚Ä¢ Dots are on or off, pixels have intensities

‚Ä¢ Types

‚Ä¢ Typewriter / Daisy wheels ‚Äì obsolete

‚Ä¢ Impact printing - dot matrix ‚Äì mostly obsolete

‚Ä¢ Inkjet ‚Äì squirts heated droplets of ink

‚Ä¢ Laser printer

‚Ä¢ Thermal wax transfer

‚Ä¢ Dye Sublimation


Printers


Creating a Gray Scale


Laser Printer Operation

1.  Dots of laser light are beamed onto a drum

2.  Drum becomes electrically charged

3.  Drum passes through toner which then sticks to the electrically
charged places

4.  Electrically charged paper is fed toward the drum

5.  Toner is transferred from the drum to the paper

6.  The fusing system heats and melts the toner onto the paper

7.  A corona wire resets the electrical charge on the drum


Laser Printer Operation


Laser Printer Operation


Other Computer Peripherals

‚Ä¢ Scanners

‚Ä¢ Flatbed, sheet-fed, hand-held

‚Ä¢ Light is reflected off the sheet of paper

‚Ä¢ User Input Devices

‚Ä¢ Keyboard, mouse, light pens, graphics tablets

‚Ä¢ Communication Devices

‚Ä¢ Telephone modems

‚Ä¢ Network devices


Network Communication Devices

‚Ä¢ Network is just another I/O device

‚Ä¢ Network I/O controller is the network interface card (NIC)

‚Ä¢ Types of network connections

‚Ä¢ Ethernet, FDDI(Fiber Distributed Data Interface) fiber,
token-ring

‚Ä¢ Medium access control (MAC) protocols

‚Ä¢ Define the specific rules of communication for the network


Copyright 2010 John Wiley & Sons

All rights reserved. Reproduction or translation of this work
beyond that permitted in section 117 of the 1976 United
States Copyright Act without express permission of the
copyright owner is unlawful. Request for further information
should be addressed to the Permissions Department, John
Wiley & Sons, Inc. The purchaser may make back-up copies
for his/her own use only and not for distribution or resale.

The Publisher assumes no responsibility for errors,

omissions, or damages caused by the use of these programs
or from the use of the information contained herein.‚Äù


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


Instruction set architecture
ISA

CISC VS RISC


What is an Instruction Set?

‚Ä¢ The complete collection of instructions that are understood by a CPU

‚Ä¢ Machine Code

‚Ä¢ Binary

‚Ä¢ Usually represented by assembly codes


Instruction Set Architecture (ISA)

Serves as an interface between software and  ‚Ä¢

hardware.

Provides a mechanism by which the software  ‚Ä¢

tells the hardware what should be done.

High level language code : C, C++, Java, Fortran,

compiler

Assembly language code: architecture specific statements 

assembler

Machine language code: architecture specific bit patterns 

software

 instruction set 
hardware


Elements of an Instruction

‚Ä¢ Operation code (Op code)

‚Ä¢ Do this

‚Ä¢ Source Operand reference

‚Ä¢ To this

‚Ä¢ Result Operand reference

‚Ä¢ Put the answer here

‚Ä¢ Next Instruction Reference

‚Ä¢ When you have done that, do this...


Instruction Cycle State Diagram


Instruction Representation

‚Ä¢ In machine code each instruction has a unique bit pattern

‚Ä¢ For human consumption (well, programmers anyway) a symbolic
representation is used

‚Ä¢ e.g. ADD, SUB, LOAD

‚Ä¢ Operands can also be represented in this way

‚Ä¢ ADD A,B


Simple Instruction Format


Instruction Types

‚Ä¢ Data processing

‚Ä¢ Data storage (main memory)

‚Ä¢ Data movement (I/O)

‚Ä¢ Program flow control


Number of Addresses (a)

‚Ä¢ 3 addresses                       a=(a*b+c)-d*e

‚Ä¢ Operand 1, Operand 2, Result

‚Ä¢ a = b + c;

‚Ä¢ May be a forth - next instruction (usually implicit)

‚Ä¢ Not common

‚Ä¢ Needs very long words to hold everything


Number of Addresses (b)

‚Ä¢ 2 addresses                        a=(a*b+c)-d*e

‚Ä¢ One address doubles as operand and result

‚Ä¢ a = a + b

‚Ä¢ Reduces length of instruction

‚Ä¢ Requires some extra work

‚Ä¢ Temporary storage to hold some results


Number of Addresses (c)

‚Ä¢ 1 address                              a=(a*b+c)-d*e

‚Ä¢ Implicit second address

‚Ä¢ Usually a register (accumulator)

‚Ä¢ Common on early machines


Number of Addresses (d)

‚Ä¢ 0 (zero) addresses                       a=a*b+(c-(d*e))

‚Ä¢ All addresses implicit

‚Ä¢ Uses a stack

‚Ä¢ e.g. push a

‚Ä¢   push b

‚Ä¢   add

‚Ä¢   pop c

‚Ä¢ c = a + b


How Many Addresses

‚Ä¢ More addresses

‚Ä¢ More complex (powerful?) instructions

‚Ä¢ More registers

‚Ä¢ Inter-register operations are quicker

‚Ä¢ Fewer instructions per program

‚Ä¢ Fewer addresses

‚Ä¢ Less complex (powerful?) instructions

‚Ä¢ More instructions per program

‚Ä¢ Faster fetch/execution of instructions


Design Decisions (1)

‚Ä¢ Operation repertoire

‚Ä¢ How many ops?

‚Ä¢ What can they do?

‚Ä¢ How complex are they?

‚Ä¢ Data types

‚Ä¢ Instruction formats

‚Ä¢ Length of op code field

‚Ä¢ Number of addresses


Design Decisions (2)

‚Ä¢ Registers

‚Ä¢ Number of CPU registers available

‚Ä¢ Which operations can be performed on which registers?

‚Ä¢ Addressing modes

‚Ä¢ RISC v CISC


History of RISC/CISC

‚Ä¢ 1950s IBM instituted a research program

‚Ä¢ 1964 Release of System/360

‚Ä¢ Mid-1970s improved measurement tools demonstrated on CISC

‚Ä¢ 1975 801 project initiated at IBM‚Äôs Watson Research Center

‚Ä¢ 1979 32-bit RISC microprocessor (801) developed led by Joel Birnbaum

‚Ä¢ 1984 MIPS developed at Stanford, as well as projects done at Berkeley

‚Ä¢ 1988 RISC processors had taken over high-end of the workstation market

‚Ä¢ Early 1990s IBM‚Äôs POWER (Performance Optimization With Enhanced RISC) architecture introduced w/ 
the
RISC System/6k

‚Ä¢ AIM (Apple, IBM, Motorola) alliance formed, resulting in PowerPC


What is CISC?

‚Ä¢ CISC is an acronym for Complex Instruction Set Computer and are chips that are easy to program 
and which
make efficient use of memory. Since the earliest machines were programmed in assembly language and
memory was slow and expensive, the CISC philosophy made sense, and was commonly implemented in such
large computers as the PDP-11 and the DECsystem 10 and 20 machines.

‚Ä¢ Most common microprocessor designs such as the Intel 80x86 and Motorola 68K series followed the 
CISC
philosophy.

‚Ä¢ But recent changes in software and hardware technology have forced a re-examination of CISC and 
many

modern CISC processors are hybrids, implementing many RISC principles.

‚Ä¢ CISC was developed to make compiler development simpler. It shifts most of the burden of 
generating
machine instructions to the processor. For example, instead of having to make a compiler write long 
machine
instructions to calculate a square-root, a CISC processor would have a built-in ability to do this.


CISC Attributes

The design constraints that led to the development of CISC (small amounts of slow memory and fact 
that most
early machines were programmed in assembly language) give CISC instructions sets some common
characteristics:

‚Ä¢ A 2-operand format, where instructions have a source and a destination. Register to register, 
register to
memory, and memory to register commands. Multiple addressing modes for memory, including specialized
modes for indexing through arrays

‚Ä¢ Variable length instructions where the length often varies according to the addressing mode

‚Ä¢ Instructions which require multiple clock cycles to execute.

E.g. Pentium is considered a modern CISC processor


CISC Attributes

Most CISC hardware architectures have several characteristics in common:

‚Ä¢ Complex instruction-decoding logic, driven by the need for a single
instruction to support multiple addressing modes.

‚Ä¢ A small number of general purpose registers. This is the direct result of
having instructions which can operate directly on memory and the limited
amount of chip space not dedicated to instruction decoding, execution, and
microcode storage.

‚Ä¢ Several special purpose registers. Many CTSC designs set aside special
registers for the stack pointer, interrupt handling, and so on. This can
simplify the hardware design somewhat, at the expense of making the
instruction set more complex.

‚Ä¢ A 'Condition code" register which is set as a side-effect of most instructions.
This register reflects whether the result of the last operation is less than,
equal to, or greater than zero and records if certain error conditions occur.


CISC Attributes

At the time of their initial development, CISC machines used available technologies to optimize 
computer

performance.

‚Ä¢ Microprogramming is as easy as assembly language to implement, and much less expensive than
hardwiring a control unit.

‚Ä¢ The ease of micro coding new instructions allowed designers to make CISC machines upwardly
compatible: a new computer could run the same programs as earlier computers because the new
computer would contain a superset of the instructions of the earlier computers.

‚Ä¢ As each instruction became more capable, fewer instructions could be used to implement a given 
task.
This made more efficient use of the relatively slow main memory.

‚Ä¢ Because microprogram instruction sets can be written to match the constructs of high-level 
languages,

the compiler does not have to be as complicated.


CISC Disadvantages

Designers soon realised that the CISC philosophy had its own problems, including:

‚Ä¢ Earlier generations of a processor family generally were contained as a subset in every new 
version - so

instruction set & chip hardware become more complex with each generation of computers.

‚Ä¢ So that as many instructions as possible could be stored in memory with the least possible wasted 
space,
individual instructions could be of almost any length - this means that different instructions will 
take different
amounts of clock time to execute, slowing down the overall performance of the machine.

‚Ä¢ Many specialized instructions aren't used frequently enough to justify their existence 
-approximately 20% of

the available instructions are used in a typical program.

‚Ä¢ CISC instructions typically set the condition codes as a side effect of the instruction. Not only 
does setting the
condition codes take time, but programmers have to remember to examine the condition code bits 
before a
subsequent instruction changes them.


What is RISC?

‚Ä¢ RISC?

RISC, or Reduced Instruction Set Computer. is a type of microprocessor architecture that utilizes a 
small,
highly-optimized set of instructions, rather than a more specialized set of instructions often 
found in other
types of architectures.

‚Ä¢ History

The first RISC projects came from IBM, Stanford, and UC-Berkeley in the late 70s and early 80s. The 
IBM 801,
Stanford MIPS, and Berkeley RISC 1 and 2 were all designed with a similar philosophy which has 
become
known as RISC. Certain design features have been characteristic of most RISC processors:

‚Ä¢  one cycle execution time: RISC processors have a CPI (clock per instruction) of one cycle. This 
is due to the

optimization of each instruction on the CPU and a technique called PIPELINING

‚Ä¢  pipelining: a techique that allows for simultaneous execution of parts, or stages, of 
instructions to more efficiently
process instructions;

‚Ä¢  large number of registers: the RISC design philosophy generally incorporates a larger number of 
registers to prevent
in large amounts of interactions with memory


RISC Attributes

The main characteristics of CISC microprocessors are:

‚Ä¢ Extensive instructions.

‚Ä¢ Complex and efficient machine instructions.

‚Ä¢ Microencoding of the machine instructions.

‚Ä¢ Extensive addressing capabilities for memory operations.

‚Ä¢ Relatively few registers.

In comparison, RISC processors are more or less the opposite of the above:

‚Ä¢ Reduced instruction set.

‚Ä¢ Less complex, simple instructions.

‚Ä¢ Hardwired control unit and machine instructions.

‚Ä¢ Few addressing schemes for memory operands with only two basic instructions, LOAD and

‚Ä¢ STORE

‚Ä¢ Many symmetric registers which are organised into a register file.


Pipelining

RISC Pipelines

A RISC processor pipeline operates in much the same way,
although the stages in the pipeline are different. While different
processors have different numbers of steps, they are basically
variations of these five, used in the MIPS R3000 processor:

- fetch instructions from memory

- read registers and decode the instruction

- execute the instruction or calculate an address

- access an operand in data memory

- write the result into a register


RISC Disadvantages

‚Ä¢ There is still considerable controversy among experts about the ultimate value of RISC
architectures. Its proponents argue that RISC machines are both cheaper and faster, and are
therefore the machines of the future.

‚Ä¢ However, by making the hardware simpler, RISC architectures put a greater burden on the
software. Is this worth the trouble because conventional microprocessors are becoming
increasingly fast and cheap anyway?


CISC versus RISC


CISC

Emphasis on hardware
Includes multi-clock

complex instructions

Memory-to-memory:

"LOAD" and "STORE"

incorporated in instructions

Small code sizes,

high cycles per second

Transistors used for storing
complex instructions

RISC

Emphasis on software
Single-clock,

reduced instruction only

Register to register:

"LOAD" and "STORE"

are independent instructions

Low cycles per second,

large code sizes

Spends more transistors
on memory registers


Summation

‚Ä¢ As memory speed increased, and high-level languages displaced assembly language, the major 
reasons for
CISC began to disappear, and computer designers began to look at ways computer performance could be
optimized beyond just making faster hardware.

‚Ä¢ One of their key realizations was that a sequence of simple instructions produces the same 
results as a
sequence of complex instructions, but can be implemented with a simpler (and faster) hardware 
design.
(Assuming that memory can keep up.) RISC (Reduced Instruction Set Computers) processors were the 
result.

‚Ä¢  CISC and RISC implementations are becoming more and more alike. Many of today‚Äôs RISC chips 
support as
many                                                                      instructions as 
yesterday's CISC chips. And today's CISC chips use many techniques formerly associated
with RISC chips.

‚Ä¢ To some extent, the argument is becoming moot because CISC and RISC implementations are becoming 
more
and more alike. Many of today's RISC chips support as many instructions as yesterday's CISC chips. 
And
today's CISC chips use many techniques formerly associated with RISC chips.


Thank You



College of Computing and Informatics
Computer Organization


The Architecture of Computer Hardware,

Systems Software & Networking:

An Information Technology Approach

4th Edition, Irv Englander
John Wiley and Sons ¬©2010

PowerPoint slides authored by Wilson Wong,
Bentley University

PowerPoint slides for the 3 ≥·µà edition were co-
authored with Lynne Senne, Bentley University


Instruction Set Architectures


Instruction Set Architectures

‚Ä¢ ISA determines instruction formats

‚Ä¢ The LMC is a one-address architecture (an accumulator-based
machine).


Instruction Set Architectures

‚Ä¢ ISA determines instruction formats

‚Ä¢ The LMC is a one-address architecture (an accumulator-based
machine).

‚Ä¢ e.g., the instruction ADD X

ADD takes two operands. One is
implicit (the accumulator). The
other is an address (location).


Instruction Set Architectures

‚Ä¢ ISA determines instruction formats

‚Ä¢ There are other instruction set architectures, all based on the
number of explicit operands.

‚Ä¢ 0-address (stack)

‚Ä¢ 1-address (accumulator)

‚Ä¢ 2-address

‚Ä¢ 3-address


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are implicit on the stack. Only
push/pop reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

Stack


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

Code  # Memory Refs

Stack


‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

a

POP A    1


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

Code  # Memory Refs

Stack


‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

b

ADD    0

a

POP A    1


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop


reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

a * b

Stack


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are


implicit on the stack. Only push/pop

reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

c

a * b

Stack


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop


reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

d
c

a * b

Stack


Instruction Set Architectures


‚Ä¢ 0-Address Machines

Code  # Memory Refs

Stack

‚Ä¢ All operands for binary operations are

PUSH A    1

implicit on the stack. Only push/popPUSH B    1


reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

e
d
c

a * b


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop


reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

d * e
c

a * b

Stack


Instruction Set Architectures

‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are


implicit on the stack. Only push/pop

reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

c ‚Äì (d * e)
a * b

Stack


Instruction Set Architectures

‚Ä¢ 0-Address Machines


‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

Stack

a * b + (c ‚Äì (d * e))


Instruction Set Architectures

‚Ä¢ 0-Address Machines


‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code  # Memory Refs

PUSH A    1

PUSH B    1

MUL    0

PUSH C    1

PUSH D    1

PUSH E    1

MUL    0

SUB    0

ADD    0

POP A    1

Stack


Instruction Set Architectures


‚Ä¢ 0-Address Machines

‚Ä¢ All operands for binary operations are
implicit on the stack. Only push/pop
reference memory.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code

PUSH A
PUSH B

MUL
PUSH C
PUSH D
PUSH E

MUL

# Memory Refs

1

1

0

1

1

1

0

In a stack-based
machine, the stack is
typically a set of very
fast registers,
minimizing trips to

memory; 6 memory

Stack


SUB

ADD

POP A

‚Å∞   accesses, not including

‚Å∞  instruction fetch.

1


Instruction Set Architectures

‚Ä¢ 1-Address Machines

‚Ä¢ Accumulator is a source and
destination. Second source is
explicit.


Instruction Set Architectures

‚Ä¢ 1-Address Machines

‚Ä¢ Accumulator is a source and
destination. Second source is

Code      # Memory Refs

LOAD A     1

MUL B     1

ADD C     1


explicit.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

STORE T1     1

LOAD D     1

MUL E     1

STORE T2     1

LOAD T1     1

SUB T2     1

STORE A     1


Instruction Set Architectures


‚Ä¢ 1-Address Machines

‚Ä¢ Accumulator is a source and
destination. Second source is
explicit.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code      # Memory Refs

LOAD A     1

MUL B     1

ADD C     1

STORE T1     1

LOAD D     1

MUL E     1

STORE T2     1

LOAD T1     1

SUB T2     1

STORE A     1

10 memory
references, not
including
instruction         fetch.


Instruction Set Architectures

‚Ä¢ 2-Address Machines

‚Ä¢ Two source addresses for operands.
One source is also the destination.

Addr1 is both source

and destination.


Instruction Set Architectures


‚Ä¢ 2-Address Machines

‚Ä¢ Two source addresses for operands.
One source is also the destination.

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code      # Memory Refs

MOVE T1, A     2

MUL T1, B     3

ADD T1, C     3

MOVE T2, D     2

MUL T2, E     3

SUB T1, T2     3

MOVE A, T1     2


Instruction Set Architectures

‚Ä¢ 2-Address Machines

‚Ä¢ Two source addresses for operands.
One source is also the destination.


‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code      # Memory Refs

MOVE T1, A     2

MUL T1, B     3

ADD T1, C     3

MOVE T2, D     2

MUL T2, E     3

SUB T1, T2     3

MOVE A, T1     2

Using memory-to-
memory
operations, 18
memory accesses
(not including
instruction fetch).

What if T1 and T2

were registers?


Instruction Set Architectures

‚Ä¢ 2-Address Machines

‚Ä¢ Two source addresses for operands.
One source is also the destination.


‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code      # Memory Refs

MOVE R1, A     1

MUL R1, B     1

ADD R1, C     1

MOVE R2, D     1

MUL R2, E     1

SUB R1, T2     0

MOVE A, R1     1

Cuts memory
references down
to     6. This is called
a 1¬Ω address
machine with a
load/store
architecture.


Instruction Set Architectures

‚Ä¢ 3-Address Machines

‚Ä¢ One destination operand,
two source operands,

all explicit


Instruction Set Architectures


‚Ä¢ 3-Address Machines

‚Ä¢ One destination operand,
two source operands,

all explicit

‚Ä¢ e.g., calculating

a = a * b + c ‚Äì d * e

Code      # Memory Refs

MPY T1, A, B     3

ADD T1, T1,     3

C

MPY T2, D, E     3

SUB A, T1,     3

T2


Instruction Set Architectures


‚Ä¢ 3-Address Machines

‚Ä¢ One destination operand,
two source operands,

all explicit

‚Ä¢

Code      # Memory Refs

MPY T1, A, B     3

ADD T1, T1,     3

C

12 memory accesses,
not including

instruction fetch.


e.g., calculating

a = a * b + c ‚Äì d * e

MPY T2, D, E     3

SUB A, T1,     3

T2

What if T1, T2 were
registers?


Instruction Set Architectures


‚Ä¢ 3-Address Machines

‚Ä¢ One destination operand,

Code      # Memory Refs

MPY R1, A, B     2


two source operands,

all explicit

ADD R1, R1,

C

1     6 memory accesses;

general purpose


‚Ä¢ e.g., calculating

MPY R2, D, E     2

registers make a


a = a * b + c ‚Äì d * e

SUB A, R1,

R2

1     substantial difference.


Instruction Set Architectures

‚Ä¢ Comparison

‚Ä¢ Assume 8 registers (3 bits),

32 op-codes (5 bits),

15-bit addresses,

16-bit integers.

‚Ä¢ Which ISA accesses
memory the least?


Instruction Set Architectures

‚Ä¢ Comparison

‚Ä¢ Assume 8 registers (3 bits),

Instructions          Data refs     Total


32 op-codes (5 bits),

15-bit addresses,

16-bit integers.

0-address   10 x 20 bits = 200

bits

1-address   10 x 20 bits = 200

bits

6 x 16 bits = 96 bits   296 bits

10 x 16 bits = 160 bits   360 bits


‚Ä¢ Which ISA accesses
memory the least?

1¬Ω-address  7 x 23 bits = 161 bits    6 x 16 bits = 96 bits   257 bits


2 address    7 x 35 bits = 245

bits

3-address    4 x 50 bits = 200

bits

3-address (regs)    4 x 38 bits = 152

bits

18 x 16 bits = 288 bits   519 bits

12 x 16 bits =192 bits  392 bits

6 x 16 bits = 96 bits   248 bits


Instruction Set ArchitecTtwuorcleeasr winners:

1¬Ω‚Äêaddress (RISC) and

3‚Äêaddress with


‚Ä¢ Comparison

registers (CInISstCru)c.tions        Data refs    Total


‚Ä¢ Assume 8 registers (3 bits),

32 op-codes (5 bits),

15-bit addresses,

16-bit integers.

0-address    10 x 20 bits = 200

bits

1-address    10 x 20 bits = 200

bits

6 x 16 bits = 96 bits   296 bits

10 x 16 bits = 160 bits   360 bits


‚Ä¢ Which ISA accesses

memory the least?

1¬Ω-address   7 x 23 bits = 161 bits    6 x 16 bits = 96 bits   257 bits

2 address  7 x 35 bits = 245 bits   18 x 16 bits = 288 bits   519 bits

3-address  4 x 50 bits = 200 bits    12 x 16 bits =192 bits   392 bits

3-address (regs)   4 x 38 bits = 152 bits     6 x 16 bits = 96 bits   248 bits


Instruction Set Architectures

‚Ä¢ Summary

‚Ä¢ The instruction set architecture determines the format of instructions
(and therefore the assembly language).

‚Ä¢ Four basic types with variations:

‚Ä¢ 0-address (stack)

‚Ä¢ 1-address (accumulator)

‚Ä¢ 2-address (register variant is 1¬Ω-address)

‚Ä¢ 3-address (with register variant)

‚Ä¢ ISA dramatically affects the number of times memory is accessed.


Thank You

